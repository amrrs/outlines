{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"get_started/","title":"Get Started","text":"<p>Generate text that machines understand.</p>"},{"location":"get_started/#_1","title":"Get Started","text":""},{"location":"get_started/#features","title":"Features","text":"<ul> <li> Prompting utilities</li> <li> Regex-guided generation</li> <li> JSON-guided generation</li> <li> Multiple sequence sampling methods</li> <li> Integration with several open source libraries</li> </ul>"},{"location":"get_started/#install","title":"Install","text":"<pre><code>pip install outlines\n</code></pre> Using OpenAI and Transformers <p>Outlines  does not install the <code>openai</code> or <code>transformers</code> libraries by default. You will have to install these libraries manually. To use <code>transformers</code> models you will also need to install the <code>datasets</code> library.</p>"},{"location":"get_started/#sneak-peek","title":"Sneak Peek","text":"CodeOutput <pre><code>from enum import Enum\nfrom pydantic import BaseModel, constr\n\nimport outlines.models as models\nimport outlines.text.generate as generate\n\n\nclass Weapon(str, Enum):\n    sword = \"sword\"\n    axe = \"axe\"\n    mace = \"mace\"\n    spear = \"spear\"\n    bow = \"bow\"\n    crossbow = \"crossbow\"\n\n\nclass Armor(str, Enum):\n    leather = \"leather\"\n    chainmail = \"chainmail\"\n    plate = \"plate\"\n\n\nclass Character(BaseModel):\n    name: constr(max_length=20)\n    age: int\n    armor: Armor\n    weapon: Weapon\n    strength: int\n\n\nmodel = models.transformers(\"gpt2\")\ngenerator = generate.json(model, Character)\nsequence = generator(\"Create a character description for a role playing game in JSON\")\n\nprint(sequence)\n</code></pre> <p>```json {   \"name\": \"Anonymous Tokens\",   \"age\": 7,   \"armor\": \"plate\",   \"weapon\": \"mace\",   \"strength\": 4171 } `````</p>"},{"location":"get_started/#acknowledgements","title":"Acknowledgements","text":"<p>Outlines was originally developed at @NormalComputing by @remilouf and @BrandonTWillard.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/continuation/","title":"Continuation","text":""},{"location":"api/continuation/#outlines.text.generate.continuation.Continuation","title":"<code>Continuation</code>","text":"<p>             Bases: <code>Sequence</code></p> <p>Represents a completion generation model.</p> <p><code>Continuation</code> instances are unconstrained generation models that stop when an EOS token has been found or when the maximum number of tokens has been reached.</p> <p>import outlines.text as text sequence = text.generate.continuation(model)(\"Say something\")</p> Source code in <code>outlines/text/generate/continuation.py</code> <pre><code>class Continuation(Sequence):\n    \"\"\"Represents a completion generation model.\n\n    `Continuation` instances are unconstrained generation models that stop when\n    an EOS token has been found or when the maximum number of tokens has been\n    reached.\n\n    &gt;&gt;&gt; import outlines.text as text\n    &gt;&gt;&gt; sequence = text.generate.continuation(model)(\"Say something\")\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model,\n        max_tokens: Optional[int] = None,\n        sampler: Optional[\"Sampler\"] = None,\n        stop: Union[str, List[str]] = [],\n    ):\n        super().__init__(model, max_tokens, sampler)\n        self.eos_token_id = torch.tensor(\n            [self.model.tokenizer.eos_token_id], device=self.device\n        )\n\n        if isinstance(stop, str):\n            stop = [stop]\n\n        self.stop_sequences = stop\n\n    def is_finished(self, token_ids: torch.LongTensor) -&gt; torch.BoolTensor:\n        \"\"\"Determine whether the sequences reached maximum length of end with\n        and EOS token.\n\n        We only need to look for the EOS token in the last element rather than\n        in the whole sequence. Indeed, (1) EOS is a single token (2)\n        `Sequence`'s `__call__` methods only passed the `token_ids` of the\n        sequences that haven't been marked as finished already.\n\n        Parameters\n        ----------\n        token_ids\n            The input sequences.\n\n        \"\"\"\n\n        contains_eos = token_ids[:, -1] == self.model.tokenizer.eos_token_id\n\n        if self.stop_sequences:\n            sequences = self.model.tokenizer.decode(token_ids)\n            contains_stop_sequence = []\n            for sequence in sequences:\n                contains_stop_sequence.append(\n                    any(stop_str in sequence for stop_str in self.stop_sequences)\n                )\n            contains_stop_sequence = torch.tensor(\n                contains_stop_sequence, dtype=torch.bool, device=self.model.device\n            )\n\n            return torch.logical_or(contains_eos, contains_stop_sequence)\n        else:\n            return contains_eos\n\n    def postprocess_completions(self, completions: List[str]) -&gt; List[str]:\n        \"\"\"Remove the EOS token from the completion.\n\n        Sequences in `stop` take precedence over EOS. For instance, if\n        `stop=[\"\\n\"]` and the generated sequence is 'One\\nTwo&lt;EOS&gt;`\n        `Continuation.postprocess_completions` will return `One`.\n\n        \"\"\"\n        completions_without_eos = [\n            completion.replace(self.model.tokenizer.eos_token, \"\")\n            for completion in completions\n        ]\n\n        completions_without_stop = []\n        for completion in completions_without_eos:\n            for stop_str in self.stop_sequences:\n                idx = completion.rfind(stop_str)  # ignore the prompt\n                if idx &gt; 0:\n                    completion = completion[:idx]\n\n            completions_without_stop.append(completion)\n\n        return completions_without_stop\n</code></pre>"},{"location":"api/continuation/#outlines.text.generate.continuation.Continuation.is_finished","title":"<code>is_finished(token_ids)</code>","text":"<p>Determine whether the sequences reached maximum length of end with and EOS token.</p> <p>We only need to look for the EOS token in the last element rather than in the whole sequence. Indeed, (1) EOS is a single token (2) <code>Sequence</code>'s <code>__call__</code> methods only passed the <code>token_ids</code> of the sequences that haven't been marked as finished already.</p>"},{"location":"api/continuation/#outlines.text.generate.continuation.Continuation.is_finished--parameters","title":"Parameters","text":"<p>token_ids     The input sequences.</p> Source code in <code>outlines/text/generate/continuation.py</code> <pre><code>def is_finished(self, token_ids: torch.LongTensor) -&gt; torch.BoolTensor:\n    \"\"\"Determine whether the sequences reached maximum length of end with\n    and EOS token.\n\n    We only need to look for the EOS token in the last element rather than\n    in the whole sequence. Indeed, (1) EOS is a single token (2)\n    `Sequence`'s `__call__` methods only passed the `token_ids` of the\n    sequences that haven't been marked as finished already.\n\n    Parameters\n    ----------\n    token_ids\n        The input sequences.\n\n    \"\"\"\n\n    contains_eos = token_ids[:, -1] == self.model.tokenizer.eos_token_id\n\n    if self.stop_sequences:\n        sequences = self.model.tokenizer.decode(token_ids)\n        contains_stop_sequence = []\n        for sequence in sequences:\n            contains_stop_sequence.append(\n                any(stop_str in sequence for stop_str in self.stop_sequences)\n            )\n        contains_stop_sequence = torch.tensor(\n            contains_stop_sequence, dtype=torch.bool, device=self.model.device\n        )\n\n        return torch.logical_or(contains_eos, contains_stop_sequence)\n    else:\n        return contains_eos\n</code></pre>"},{"location":"api/continuation/#outlines.text.generate.continuation.Continuation.postprocess_completions","title":"<code>postprocess_completions(completions)</code>","text":"<p>Remove the EOS token from the completion.</p> <pre><code>    Sequences in `stop` take precedence over EOS. For instance, if\n    `stop=[\"\n</code></pre> <p>\"]<code>and the generated sequence is 'One Two&lt;EOS&gt;</code> <code>Continuation.postprocess_completions</code> will return <code>One</code>.</p> Source code in <code>outlines/text/generate/continuation.py</code> <pre><code>def postprocess_completions(self, completions: List[str]) -&gt; List[str]:\n    \"\"\"Remove the EOS token from the completion.\n\n    Sequences in `stop` take precedence over EOS. For instance, if\n    `stop=[\"\\n\"]` and the generated sequence is 'One\\nTwo&lt;EOS&gt;`\n    `Continuation.postprocess_completions` will return `One`.\n\n    \"\"\"\n    completions_without_eos = [\n        completion.replace(self.model.tokenizer.eos_token, \"\")\n        for completion in completions\n    ]\n\n    completions_without_stop = []\n    for completion in completions_without_eos:\n        for stop_str in self.stop_sequences:\n            idx = completion.rfind(stop_str)  # ignore the prompt\n            if idx &gt; 0:\n                completion = completion[:idx]\n\n        completions_without_stop.append(completion)\n\n    return completions_without_stop\n</code></pre>"},{"location":"api/continuation/#outlines.text.generate.continuation.continuation","title":"<code>continuation(model, max_tokens=None, *, sampler=None, stop=[])</code>","text":"<p>Generate text sequences.</p>"},{"location":"api/continuation/#outlines.text.generate.continuation.continuation--parameters","title":"Parameters","text":"<p>model     The language model to use to compute the next-token logits. max_tokens     The maximum number of tokens to generate. sampler     The function used to draw samples.  Defaults to     <code>outlines.text.generate.sample.multinomial</code>.  See     <code>outlines.text.generate.sample.Sampler</code> for the expected form of     such functions. stop     A string or list of strings which, when generated, stops     the generation for this sequence.</p> Source code in <code>outlines/text/generate/continuation.py</code> <pre><code>def continuation(\n    model,\n    max_tokens: Optional[int] = None,\n    *,\n    sampler: Optional[\"Sampler\"] = None,\n    stop: Union[str, List[str]] = [],\n):\n    \"\"\"Generate text sequences.\n\n    Parameters\n    ----------\n    model\n        The language model to use to compute the next-token logits.\n    max_tokens\n        The maximum number of tokens to generate.\n    sampler\n        The function used to draw samples.  Defaults to\n        `outlines.text.generate.sample.multinomial`.  See\n        `outlines.text.generate.sample.Sampler` for the expected form of\n        such functions.\n    stop\n        A string or list of strings which, when generated, stops\n        the generation for this sequence.\n\n    \"\"\"\n    return Continuation(model, max_tokens, sampler, stop)\n</code></pre>"},{"location":"api/fsm/","title":"Fsm","text":""},{"location":"api/fsm/#outlines.text.fsm.create_fsm_index","title":"<code>create_fsm_index(fsm_info, vocabulary, final_state_string=None, n_jobs=-1)</code>","text":"<p>Construct a map from FSM states to subsets of <code>vocabulary</code>.</p> <p>The subsets of <code>vocabulary</code> consist of elements that are accepted by--or transition to--the corresponding partial parse states.</p>"},{"location":"api/fsm/#outlines.text.fsm.create_fsm_index--parameters","title":"Parameters","text":"<p>fsm     The finite-state machine. vocabulary     The vocabulary composed of token strings mapped to token IDs. final_state_string     A string from <code>vocabulary</code> that is to be added to all the final states     in the FSM (e.g. <code>\"&lt;EOS&gt;\"</code>).</p> Source code in <code>outlines/text/fsm.py</code> <pre><code>def create_fsm_index(\n    fsm_info: FSMInfo,\n    vocabulary: Dict[str, int],\n    final_state_string: Optional[str] = None,\n    n_jobs=-1,\n) -&gt; Dict[int, Set[int]]:\n    \"\"\"Construct a map from FSM states to subsets of `vocabulary`.\n\n    The subsets of `vocabulary` consist of elements that are accepted by--or\n    transition to--the corresponding partial parse states.\n\n    Parameters\n    ----------\n    fsm\n        The finite-state machine.\n    vocabulary\n        The vocabulary composed of token strings mapped to token IDs.\n    final_state_string\n        A string from `vocabulary` that is to be added to all the final states\n        in the FSM (e.g. ``\"&lt;EOS&gt;\"``).\n    \"\"\"\n\n    results = Parallel(backend=\"threading\", n_jobs=n_jobs, return_as=\"generator\")(\n        delayed(process_token_string)(fsm_info, token, token_idx, final_state_string)\n        for token, token_idx in vocabulary.items()\n    )\n\n    states_to_token_subsets: Dict[int, Set[int]] = {}\n\n    for fsm_state, token_idx in chain.from_iterable(results):\n        states_to_token_subsets.setdefault(fsm_state, set()).add(token_idx)\n\n    return states_to_token_subsets\n</code></pre>"},{"location":"api/fsm/#outlines.text.fsm.create_fsm_index_end_to_end","title":"<code>create_fsm_index_end_to_end(fsm_info, vocabulary)</code>","text":"<p>Create an FSM state-to-vocabulary map/index through end-to-end token parsing.</p> Source code in <code>outlines/text/fsm.py</code> <pre><code>def create_fsm_index_end_to_end(\n    fsm_info: FSMInfo,\n    vocabulary: Dict[str, List[int]],\n) -&gt; Dict[int, Set[Tuple[int, int]]]:\n    \"\"\"Create an FSM state-to-vocabulary map/index through end-to-end token parsing.\"\"\"\n\n    # TODO: Consider using a `List` of `Set`s instead; that way we can JIT this\n    # code, too.\n    states_to_token_subsets: Dict[int, Set[Tuple[int, int]]] = {}\n    seen: Set[int] = set()\n    next_states = {fsm_info.initial}\n\n    while next_states:\n        start_state = next_states.pop()\n\n        token_ids_end_states = state_scan_tokens(\n            fsm_info.transitions,\n            fsm_info.alphabet_symbol_mapping,\n            fsm_info.alphabet_anything_value,\n            fsm_info.initial,\n            fsm_info.finals,\n            vocabulary,\n            start_state,\n        )\n\n        for token_id_and_end_state in token_ids_end_states:\n            states_to_token_subsets.setdefault(start_state, set()).add(\n                token_id_and_end_state\n            )\n            end_state = token_id_and_end_state[1]\n            if end_state not in seen:\n                next_states.add(end_state)\n\n        seen.add(start_state)\n\n    return states_to_token_subsets\n</code></pre>"},{"location":"api/fsm/#outlines.text.fsm.create_fsm_index_tokenizer","title":"<code>create_fsm_index_tokenizer(fsm, tokenizer)</code>","text":"<p>Construct an FMS index from a tokenizer.</p> <p>This uses the end-to-end approach of <code>create_fsm_index_end_to_end</code>.</p> <p>.. warning::</p> <pre><code>`fsm` needs to be deterministically ordered so that future caching makes sense.\n</code></pre> Source code in <code>outlines/text/fsm.py</code> <pre><code>def create_fsm_index_tokenizer(\n    fsm: BetterFSM,\n    tokenizer: \"Tokenizer\",\n) -&gt; Tuple[Dict[int, Dict[int, int]], Set[int]]:\n    \"\"\"Construct an FMS index from a tokenizer.\n\n    This uses the end-to-end approach of `create_fsm_index_end_to_end`.\n\n    .. warning::\n\n        `fsm` needs to be deterministically ordered so that future caching makes sense.\n\n    \"\"\"\n    vocabulary, empty_token_ids = reduced_vocabulary(tokenizer)\n\n    states_to_token_subsets = create_fsm_index_end_to_end(fsm.fsm_info, vocabulary)\n\n    # Allow transitions to EOS from all terminals FSM states that are\n    # reachable\n    # TODO: Do we really need this anymore?\n    for state in fsm.fsm_info.finals:\n        subset = states_to_token_subsets.get(state)\n        if subset is not None:\n            subset.add((tokenizer.eos_token_id, state))\n\n    # Convert to token-to-end-state maps\n    states_to_token_subsets = {k: dict(v) for k, v in states_to_token_subsets.items()}\n\n    return states_to_token_subsets, empty_token_ids\n</code></pre>"},{"location":"api/fsm/#outlines.text.fsm.find_partial_matches","title":"<code>find_partial_matches(fsm_info, input_string, full_match=True)</code>","text":"<p>Find the states in the finite state machine <code>fsm_info</code> that accept <code>input_string</code>.</p> <p>This will consider all possible states in the finite state machine (FSM) that accept the beginning of <code>input_string</code> as starting points, unless a specific <code>start_state</code> is provided.</p>"},{"location":"api/fsm/#outlines.text.fsm.find_partial_matches--parameters","title":"Parameters","text":"<p>fsm_info     The finite state machine. input_string     The string for which we generate partial matches. full_match     Matches must cover the entire string.</p>"},{"location":"api/fsm/#outlines.text.fsm.find_partial_matches--returns","title":"Returns","text":"<p>A set of tuples corresponding to each valid starting state in the FSM.  The first element of each tuple contains an integer indicating the position in <code>input_string</code> at which the FSM stopped.  The second element is the tuple of states visited during execution of the FSM plus the next, unvisited transition state.</p> Source code in <code>outlines/text/fsm.py</code> <pre><code>@numba.njit(nogil=True)\ndef find_partial_matches(\n    fsm_info: FSMInfo,\n    input_string: str,\n    full_match: bool = True,\n) -&gt; Generator[Tuple[int, List[int]], None, None]:\n    \"\"\"Find the states in the finite state machine `fsm_info` that accept `input_string`.\n\n    This will consider all possible states in the finite state machine (FSM)\n    that accept the beginning of `input_string` as starting points, unless a\n    specific `start_state` is provided.\n\n    Parameters\n    ----------\n    fsm_info\n        The finite state machine.\n    input_string\n        The string for which we generate partial matches.\n    full_match\n        Matches must cover the entire string.\n\n    Returns\n    -------\n    A set of tuples corresponding to each valid starting state in the FSM.  The\n    first element of each tuple contains an integer indicating the position in\n    `input_string` at which the FSM stopped.  The second element is the tuple\n    of states visited during execution of the FSM plus the next, unvisited\n    transition state.\n\n    \"\"\"\n\n    if len(input_string) == 0:\n        return\n\n    trans_key = fsm_info.alphabet_symbol_mapping.get(\n        input_string[0], fsm_info.alphabet_anything_value\n    )\n\n    for state in fsm_info.trans_key_to_states.get(\n        trans_key, numba.typed.List.empty_list(numba.int64)  # type: ignore\n    ):\n        path = _walk_fsm(\n            fsm_info.transitions,\n            fsm_info.alphabet_symbol_mapping,\n            fsm_info.alphabet_anything_value,\n            fsm_info.initial,\n            fsm_info.finals,\n            input_string,\n            state,\n            full_match=full_match,\n        )\n        if path:\n            path.insert(0, state)\n            res = (len(path) - 2, path)\n            yield res\n</code></pre>"},{"location":"api/fsm/#outlines.text.fsm.fsm_union","title":"<code>fsm_union(fsms)</code>","text":"<p>Construct an FSM representing the union of the FSMs in <code>fsms</code>.</p> <p>This is an updated version of <code>interegular.fsm.FSM.union</code> made to return an extra map of component FSMs to the sets of state transitions that correspond to them in the new FSM.</p> Source code in <code>outlines/text/fsm.py</code> <pre><code>def fsm_union(\n    fsms: Sequence[FSM],\n) -&gt; Tuple[FSM, Dict[int, Tuple[Set[Tuple[int, int]], Set[int], Dict[int, Set[int]]]]]:\n    \"\"\"Construct an FSM representing the union of the FSMs in `fsms`.\n\n    This is an updated version of `interegular.fsm.FSM.union` made to return an\n    extra map of component FSMs to the sets of state transitions that\n    correspond to them in the new FSM.\n\n    \"\"\"\n\n    alphabet, new_to_old = Alphabet.union(*[fsm.alphabet for fsm in fsms])\n\n    indexed_fsms = tuple(enumerate(fsms))\n\n    initial = {i: fsm.initial for (i, fsm) in indexed_fsms}\n\n    # Dedicated function accepting a \"superset\" and returning the next\n    # \"superset\" obtained by following this transition in the new FSM\n    def follow(current_state, new_transition: int):\n        next = {}\n        for i, f in indexed_fsms:\n            old_transition = new_to_old[i][new_transition]\n            if (\n                i in current_state\n                and current_state[i] in f.map\n                and old_transition in f.map[current_state[i]]\n            ):\n                next[i] = f.map[current_state[i]][old_transition]\n        if not next:\n            raise OblivionError\n        return next\n\n    states = [initial]\n    finals: Set[int] = set()\n    map: Dict[int, Dict[int, int]] = {}\n\n    # Map component FSMs to their new state-to-state transitions, finals, and a\n    # map translating component FSM states to aggregate FSM states\n    fsms_to_trans_finals: Dict[\n        int, Tuple[Set[Tuple[int, int]], Set[int], Dict[int, Set[int]]]\n    ] = {}\n\n    i = 0\n    while i &lt; len(states):\n        state = states[i]\n\n        # Add to the finals of the aggregate FSM whenever we hit a final in a\n        # component FSM\n        if any(state.get(j, -1) in fsm.finals for (j, fsm) in indexed_fsms):\n            finals.add(i)\n\n        # Compute the map for this state\n        map[i] = {}\n        for transition in alphabet.by_transition:\n            try:\n                next = follow(state, transition)\n            except OblivionError:\n                # Reached an oblivion state; don't list it\n                continue\n            else:\n                try:\n                    # TODO: Seems like this could--and should--be avoided\n                    j = states.index(next)\n                except ValueError:\n                    j = len(states)\n                    states.append(next)\n\n                map[i][transition] = j\n\n                for fsm_id, fsm_state in next.items():\n                    (\n                        fsm_transitions,\n                        fsm_finals,\n                        fsm_old_to_new,\n                    ) = fsms_to_trans_finals.setdefault(fsm_id, (set(), set(), {}))\n                    old_from = state[fsm_id]\n                    old_to = fsm_state\n                    fsm_old_to_new.setdefault(old_from, set()).add(i)\n                    fsm_old_to_new.setdefault(old_to, set()).add(j)\n                    fsm_transitions.add((i, j))\n                    if fsm_state in fsms[fsm_id].finals:\n                        fsm_finals.add(j)\n\n        i += 1\n\n    fsm = FSM(\n        alphabet=alphabet,\n        states=range(len(states)),\n        initial=0,\n        finals=finals,\n        map=map,\n        __no_validation__=True,\n    )\n\n    fsm, old_to_new_states = make_deterministic_fsm(fsm)\n    _fsms_to_trans_finals = {\n        fsm_id: (\n            {(old_to_new_states[s1], old_to_new_states[s2]) for s1, s2 in transitions},\n            {old_to_new_states[s] for s in finals},\n            {\n                old_state: {old_to_new_states[new_state] for new_state in new_states}\n                for old_state, new_states in old_to_new.items()\n            },\n        )\n        for fsm_id, (transitions, finals, old_to_new) in sorted(\n            fsms_to_trans_finals.items(), key=lambda x: x[0]\n        )\n    }\n\n    return (\n        fsm,\n        _fsms_to_trans_finals,\n    )\n</code></pre>"},{"location":"api/fsm/#outlines.text.fsm.get_sub_fsms_from_seq","title":"<code>get_sub_fsms_from_seq(state_seq, fsms_to_trans_finals)</code>","text":"<p>Get the indices of the sub-FSMs in <code>fsm</code> that could have matched the state sequence <code>state_seq</code>.</p>"},{"location":"api/fsm/#outlines.text.fsm.get_sub_fsms_from_seq--parameters","title":"Parameters","text":"<p>state_seq     A state sequence. fsms_to_trans_finals     A map from FSM indices to tuples containing sets of their state transitions     and sets of the final/accept states.</p>"},{"location":"api/fsm/#outlines.text.fsm.get_sub_fsms_from_seq--returns","title":"Returns","text":"<p>A generator returning tuples containing each sub-FSM index (in the order they were union-ed to construct <code>fsm</code>) and booleans indicating whether or not there is another valid transition from the last state in the sequence for the associated sub-FSM (i.e. if the FSM can continue accepting/matching) and whether or not the sequence ends in a final state of the sub-FSM.</p> Source code in <code>outlines/text/fsm.py</code> <pre><code>def get_sub_fsms_from_seq(\n    state_seq: Sequence[int],\n    fsms_to_trans_finals: Dict[\n        int, Tuple[Set[Tuple[int, int]], Set[int], Dict[int, Set[int]]]\n    ],\n) -&gt; Generator[Tuple[int, bool, bool], None, None]:\n    \"\"\"Get the indices of the sub-FSMs in `fsm` that could have matched the state sequence `state_seq`.\n\n    Parameters\n    ----------\n    state_seq\n        A state sequence.\n    fsms_to_trans_finals\n        A map from FSM indices to tuples containing sets of their state transitions\n        and sets of the final/accept states.\n\n    Returns\n    -------\n    A generator returning tuples containing each sub-FSM index (in the order\n    they were union-ed to construct `fsm`) and booleans indicating whether or\n    not there is another valid transition from the last state in the sequence\n    for the associated sub-FSM (i.e. if the FSM can continue\n    accepting/matching) and whether or not the sequence ends in a final state\n    of the sub-FSM.\n    \"\"\"\n    state_seq_transitions = set(zip(state_seq[:-1], state_seq[1:]))\n    last_fsm_state = state_seq[-1]\n    yield from (\n        (\n            # The sub-FMS index\n            fsm_idx,\n            # Is there another possible transition in this sub-FSM?\n            any(last_fsm_state == from_s for (from_s, to_s) in transitions),\n            # Is this sub-FSM in a final state?\n            state_seq[-1] in finals,\n        )\n        for fsm_idx, (transitions, finals, _) in fsms_to_trans_finals.items()\n        if state_seq_transitions.issubset(transitions)\n    )\n</code></pre>"},{"location":"api/fsm/#outlines.text.fsm.make_deterministic_fsm","title":"<code>make_deterministic_fsm(fsm)</code>","text":"<p>Construct an equivalent FSM with deterministic state labels.</p> Source code in <code>outlines/text/fsm.py</code> <pre><code>def make_deterministic_fsm(fsm: FSM) -&gt; Tuple[BetterFSM, Dict[int, int]]:\n    \"\"\"Construct an equivalent FSM with deterministic state labels.\"\"\"\n    old_to_new_trans_keys = {\n        trans_key: i\n        for i, (trans_key, _) in enumerate(\n            sorted(fsm.alphabet.by_transition.items(), key=lambda x: sorted(x[1]))\n        )\n    }\n\n    new_symbol_mapping = {\n        symbol: old_to_new_trans_keys[trans_key]\n        for symbol, trans_key in fsm.alphabet._symbol_mapping.items()\n    }\n\n    new_alphabet = BetterAlphabet(new_symbol_mapping)\n\n    new_map = {\n        from_state: {\n            old_to_new_trans_keys[trans_key]: to_state\n            for trans_key, to_state in trans_map.items()\n        }\n        for from_state, trans_map in fsm.map.items()\n    }\n\n    old_to_new_states = {}\n    old_to_new_states[fsm.initial] = 0\n\n    i = 0\n    seen = {fsm.initial}\n    old_state_queue = [fsm.initial]\n    while old_state_queue:\n        old_state = old_state_queue.pop(-1)\n        transitions = new_map[old_state]\n        sorted_transitions = sorted(transitions.items(), key=lambda v: v[0])\n        for _, old_state in sorted_transitions:\n            if old_state not in seen:\n                old_state_queue.append(old_state)\n                seen.add(old_state)\n            if old_state not in old_to_new_states:\n                i += 1\n                old_to_new_states[old_state] = i\n\n    new_map = dict(\n        sorted(\n            (\n                (\n                    old_to_new_states[from_state],\n                    dict(\n                        sorted(\n                            (\n                                (trans_key, old_to_new_states[to_state])\n                                for trans_key, to_state in trans_map.items()\n                            ),\n                            key=lambda v: v[0],\n                        )\n                    ),\n                )\n                for from_state, trans_map in new_map.items()\n            ),\n            key=lambda v: v[0],\n        )\n    )\n\n    new_initial = 0\n    new_finals = frozenset(\n        sorted(old_to_new_states[old_state] for old_state in fsm.finals)\n    )\n    new_states = frozenset(sorted(new_map.keys()))\n\n    new_fsm = BetterFSM(new_alphabet, new_states, new_initial, new_finals, new_map)\n\n    return new_fsm, old_to_new_states\n</code></pre>"},{"location":"api/fsm/#outlines.text.fsm.reduced_vocabulary","title":"<code>reduced_vocabulary(tokenizer)</code>  <code>cached</code>","text":"<p>Create a map from decoded vocabulary tokens to lists of equivalent token ids.</p> Source code in <code>outlines/text/fsm.py</code> <pre><code>@lru_cache\ndef reduced_vocabulary(tokenizer: \"Tokenizer\"):\n    \"\"\"Create a map from decoded vocabulary tokens to lists of equivalent token ids.\"\"\"\n    vocabulary = numba.typed.Dict.empty(\n        numba.types.string, numba.types.ListType(numba.int64)\n    )\n    empty_token_ids = set()\n    for token, token_idx in tokenizer.vocabulary.items():\n        if token in tokenizer.special_tokens:\n            continue\n\n        token_str = tokenizer.convert_token_to_string(token)\n\n        if token_str:\n            vocabulary.setdefault(\n                token_str,\n                numba.typed.List.empty_list(numba.int64),\n            ).append(numba.int64(token_idx))\n        else:\n            empty_token_ids.add(numba.int64(token_idx))\n\n    return vocabulary, empty_token_ids\n</code></pre>"},{"location":"api/json_schema/","title":"Json schema","text":""},{"location":"api/json_schema/#outlines.text.json_schema.build_regex_from_object","title":"<code>build_regex_from_object(object)</code>","text":"<p>Turn a JSON schema into a regex that matches any JSON object that follows this schema.</p> <p>JSON Schema is a declarative language that allows to annotate JSON documents with types and descriptions. These schemas can be generated from any Python datastructure that has type annotation: namedtuples, dataclasses, Pydantic models. And by ensuring that the generation respects the schema we ensure that the output can be parsed into these objects. This function parses the provided schema and builds a generation schedule which mixes deterministic generation (fixed strings), and sampling with constraints.</p>"},{"location":"api/json_schema/#outlines.text.json_schema.build_regex_from_object--parameters","title":"Parameters","text":"<p>schema     A string that represents a JSON Schema.</p>"},{"location":"api/json_schema/#outlines.text.json_schema.build_regex_from_object--returns","title":"Returns","text":"<p>A generation schedule. A list of strings that represent the JSON schema's structure and regular expression that define the structure of the fields.</p>"},{"location":"api/json_schema/#outlines.text.json_schema.build_regex_from_object--references","title":"References","text":"<p>.. [0] JSON Schema. https://json-schema.org/</p> Source code in <code>outlines/text/json_schema.py</code> <pre><code>def build_regex_from_object(object: Union[str, Callable, BaseModel]):\n    \"\"\"Turn a JSON schema into a regex that matches any JSON object that follows\n    this schema.\n\n    JSON Schema is a declarative language that allows to annotate JSON documents\n    with types and descriptions. These schemas can be generated from any Python\n    datastructure that has type annotation: namedtuples, dataclasses, Pydantic\n    models. And by ensuring that the generation respects the schema we ensure\n    that the output can be parsed into these objects.\n    This function parses the provided schema and builds a generation schedule which\n    mixes deterministic generation (fixed strings), and sampling with constraints.\n\n    Parameters\n    ----------\n    schema\n        A string that represents a JSON Schema.\n\n    Returns\n    -------\n    A generation schedule. A list of strings that represent the JSON\n    schema's structure and regular expression that define the structure of\n    the fields.\n\n    References\n    ----------\n    .. [0] JSON Schema. https://json-schema.org/\n\n    \"\"\"\n\n    if isinstance(object, type(BaseModel)):\n        schema = object.model_json_schema()\n    elif callable(object):\n        schema = get_schema_from_signature(object)\n    else:\n        schema = json.loads(object)\n\n    Validator.check_schema(schema)\n\n    # Build reference resolver\n    schema = Resource(contents=schema, specification=DRAFT202012)\n    uri = schema.id() if schema.id() is not None else \"\"\n    registry = Registry().with_resource(uri=uri, resource=schema)\n    resolver = registry.resolver()\n\n    content = schema.contents\n    return to_regex(resolver, content)\n</code></pre>"},{"location":"api/json_schema/#outlines.text.json_schema.get_schema_from_signature","title":"<code>get_schema_from_signature(fn)</code>","text":"<p>Turn a function signature into a JSON schema.</p> <p>Every JSON object valid to the output JSON Schema can be passed to <code>fn</code> using the ** unpacking syntax.</p> Source code in <code>outlines/text/json_schema.py</code> <pre><code>def get_schema_from_signature(fn: Callable) -&gt; str:\n    \"\"\"Turn a function signature into a JSON schema.\n\n    Every JSON object valid to the output JSON Schema can be passed\n    to `fn` using the ** unpacking syntax.\n\n    \"\"\"\n    signature = inspect.signature(fn)\n    arguments = {}\n    for name, arg in signature.parameters.items():\n        if arg.annotation == inspect._empty:\n            raise ValueError(\"Each argument must have a type annotation\")\n        else:\n            arguments[name] = (arg.annotation, ...)\n\n    model = create_model(\"Arguments\", **arguments)\n\n    return model.model_json_schema()\n</code></pre>"},{"location":"api/json_schema/#outlines.text.json_schema.to_regex","title":"<code>to_regex(resolver, instance)</code>","text":"<p>Translate a JSON Schema instance into a regex that validates the schema.</p>"},{"location":"api/json_schema/#outlines.text.json_schema.to_regex--note","title":"Note","text":"<p>Many features of JSON schema are missing: - Support the fact that fields in an object are optional by default - Handle <code>required</code> keyword - Handle <code>additionalProperties</code> keyword - Handle types defined as a list - Handle constraints on numbers - Handle special patterns: <code>date</code>, <code>uri</code>, etc. - Handle optional fields (not in <code>required</code>)</p> <p>This does not support recursive definitions.</p>"},{"location":"api/json_schema/#outlines.text.json_schema.to_regex--parameters","title":"Parameters","text":"<p>resolver     An object that resolves references to other instances within a schema instance     The instance to translate</p> Source code in <code>outlines/text/json_schema.py</code> <pre><code>def to_regex(resolver: Resolver, instance: dict):\n    \"\"\"Translate a JSON Schema instance into a regex that validates the schema.\n\n    Note\n    ----\n    Many features of JSON schema are missing:\n    - Support the fact that fields in an object are optional by default\n    - Handle `required` keyword\n    - Handle `additionalProperties` keyword\n    - Handle types defined as a list\n    - Handle constraints on numbers\n    - Handle special patterns: `date`, `uri`, etc.\n    - Handle optional fields (not in `required`)\n\n    This does not support recursive definitions.\n\n    Parameters\n    ----------\n    resolver\n        An object that resolves references to other instances within a schema\n    instance\n        The instance to translate\n    \"\"\"\n    whitespace = r\"[\\n ]*\"\n\n    if \"properties\" in instance:\n        regex = \"\"\n        regex += r\"\\{\"\n        for i, (name, value) in enumerate(instance[\"properties\"].items()):\n            regex += f'{whitespace}\"{name}\"{whitespace}:{whitespace}'\n            regex += to_regex(resolver, value)\n\n            # No comma after the last key-value pair in JSON\n            if i &lt; len(instance[\"properties\"]) - 1:\n                regex += f\"{whitespace},\"\n\n        regex += f\"{whitespace}\" + r\"\\}\"\n\n        return regex\n\n    # To validate against allOf, the given data must be valid against all of the\n    # given subschemas.\n    elif \"allOf\" in instance:\n        subregexes = [to_regex(resolver, t) for t in instance[\"allOf\"]]\n        subregexes_str = [f\"{subregex}\" for subregex in subregexes]\n        return rf\"({''.join(subregexes_str)})\"\n\n    # To validate against `anyOf`, the given data must be valid against\n    # any (one or more) of the given subschemas.\n    elif \"anyOf\" in instance:\n        subregexes = [to_regex(resolver, t) for t in instance[\"anyOf\"]]\n        combinations = [\n            \"(\" + \"\".join(c) + \")\"\n            for r in range(1, len(subregexes) + 1)\n            for c in it.permutations(subregexes, r)\n        ]\n\n        return rf\"({'|'.join(combinations)})\"\n\n    # To validate against oneOf, the given data must be valid against exactly\n    # one of the given subschemas.\n    elif \"oneOf\" in instance:\n        subregexes = [to_regex(resolver, t) for t in instance[\"oneOf\"]]\n        return rf\"({'|'.join(subregexes)})\"\n\n    # The enum keyword is used to restrict a value to a fixed set of values. It\n    # must be an array with at least one element, where each element is unique.\n    elif \"enum\" in instance:\n        choices = []\n        for choice in instance[\"enum\"]:\n            if type(choice) in [int, float, bool, None]:\n                choices.append(re.escape(str(choice)))\n            elif type(choice) == str:\n                choices.append(f'\"{re.escape(choice)}\"')\n\n        return f\"({'|'.join(choices)})\"\n\n    elif \"$ref\" in instance:\n        path = f\"{instance['$ref']}\"\n        instance = resolver.lookup(path).contents\n        return to_regex(resolver, instance)\n\n    # The type keyword may either be a string or an array:\n    # - If it's a string, it is the name of one of the basic types.\n    # - If it is an array, it must be an array of strings, where each string is\n    # the name of one of the basic types, and each element is unique. In this\n    # case, the JSON snippet is valid if it matches any of the given types.\n    elif \"type\" in instance:\n        instance_type = instance[\"type\"]\n        if instance_type == \"string\":\n            if \"maxLength\" in instance or \"minLength\" in instance:\n                max_items = instance.get(\"maxLength\", \"\")\n                min_items = instance.get(\"minLength\", \"\")\n                try:\n                    if int(max_items) &lt; int(min_items):\n                        raise ValueError(\n                            \"maxLength must be greater than or equal to minLength\"\n                        )\n                except ValueError:\n                    pass\n                return f'\"{STRING_INNER}{{{min_items},{max_items}}}\"'\n            elif \"pattern\" in instance:\n                pattern = instance[\"pattern\"]\n                if pattern[0] == \"^\" and pattern[-1] == \"$\":\n                    return rf'(^\"{pattern[1:-1]}\"$)'\n                else:\n                    return rf'(\"{pattern}\")'\n            else:\n                return type_to_regex[\"string\"]\n\n        elif instance_type == \"number\":\n            return type_to_regex[\"number\"]\n\n        elif instance_type == \"integer\":\n            return type_to_regex[\"integer\"]\n\n        elif instance_type == \"array\":\n            min_items = instance.get(\"minItems\", \"0\")\n            max_items = instance.get(\"maxItems\", \"\")\n            if min_items == max_items:\n                num_repeats = \"{\" + str(int(min_items) - 1) + \"}\"\n            else:\n                num_repeats = \"*\"\n\n            if \"items\" in instance:\n                items_regex = to_regex(resolver, instance[\"items\"])\n                return rf\"\\[({items_regex})(,({items_regex})){num_repeats}\\]\"\n            else:\n                # Here we need to make the choice to exclude generating list of objects\n                # if the specification of the object is not given, even though a JSON\n                # object that contains an object here would be valid under the specification.\n                types = [\n                    {\"type\": \"boolean\"},\n                    {\"type\": \"null\"},\n                    {\"type\": \"number\"},\n                    {\"type\": \"integer\"},\n                    {\"type\": \"string\"},\n                ]\n                regexes = [to_regex(resolver, t) for t in types]\n                return (\n                    rf\"\\[({'|'.join(regexes)})(,({'|'.join(regexes)})){num_repeats}\\]\"\n                )\n\n        elif instance_type == \"boolean\":\n            return type_to_regex[\"boolean\"]\n\n        elif instance_type == \"null\":\n            return type_to_regex[\"null\"]\n\n        elif isinstance(instance_type, list):\n            # Here we need to make the choice to exclude generating an object\n            # if the specification of the object is not give, even though a JSON\n            # object that contains an object here would be valid under the specification.\n            regexes = [\n                to_regex(resolver, {\"type\": t}) for t in instance_type if t != \"object\"\n            ]\n            return rf\"({'|'.join(regexes)})\"\n\n    raise NotImplementedError(\n        f\"\"\"Could not translate the instance {instance} to a\n    regular expression. Make sure it is valid to the JSON Schema specification. If\n    it is, please open an issue on the Outlines repository\"\"\"\n    )\n</code></pre>"},{"location":"api/models/","title":"Models","text":"<p>Integration with OpenAI's API.</p>"},{"location":"api/models/#outlines.models.transformers.Transformer","title":"<code>Transformer</code>","text":"<p>Represents a <code>transformers</code> model.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>class Transformer:\n    \"\"\"Represents a `transformers` model.\"\"\"\n\n    def __init__(\n        self,\n        model: \"PreTrainedModel\",\n        tokenizer: \"PreTrainedTokenizer\",\n    ):\n        self.device = model.device\n        self.model = model\n        self.tokenizer = tokenizer\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor,\n        attention_mask: torch.LongTensor,\n        past_key_values: Optional[Tuple] = None,\n    ) -&gt; Tuple[torch.FloatTensor, Optional[KVCacheType]]:\n        \"\"\"Compute a forward pass through the transformer model.\n\n        Parameters\n        ----------\n        input_ids\n            The input token ids.  Must be one or two dimensional.\n        attention_mask\n            The attention mask.  Must be one or two dimensional.\n        past_key_values\n            A tuple of tuples containing the cached key and value tensors for each\n            attention head.\n\n        Returns\n        -------\n        The computed logits and the new cached key and value tensors.\n\n        \"\"\"\n        assert 0 &lt; input_ids.ndim &lt; 3\n\n        if past_key_values:\n            input_ids = input_ids[..., -1].unsqueeze(-1)\n\n        output = self.model(\n            input_ids,\n            attention_mask=attention_mask,\n            return_dict=True,\n            output_attentions=False,\n            output_hidden_states=False,\n            past_key_values=past_key_values,\n        )\n        next_token_logits = output.logits[..., -1, :]\n\n        return next_token_logits, output.past_key_values\n\n    def __call__(\n        self,\n        input_ids: torch.LongTensor,\n        attention_mask: torch.LongTensor,\n        past_key_values: Optional[Tuple] = None,\n    ) -&gt; torch.FloatTensor:\n        return self.forward(input_ids, attention_mask, past_key_values)[0]\n</code></pre>"},{"location":"api/models/#outlines.models.transformers.Transformer.forward","title":"<code>forward(input_ids, attention_mask, past_key_values=None)</code>","text":"<p>Compute a forward pass through the transformer model.</p>"},{"location":"api/models/#outlines.models.transformers.Transformer.forward--parameters","title":"Parameters","text":"<p>input_ids     The input token ids.  Must be one or two dimensional. attention_mask     The attention mask.  Must be one or two dimensional. past_key_values     A tuple of tuples containing the cached key and value tensors for each     attention head.</p>"},{"location":"api/models/#outlines.models.transformers.Transformer.forward--returns","title":"Returns","text":"<p>The computed logits and the new cached key and value tensors.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.LongTensor,\n    attention_mask: torch.LongTensor,\n    past_key_values: Optional[Tuple] = None,\n) -&gt; Tuple[torch.FloatTensor, Optional[KVCacheType]]:\n    \"\"\"Compute a forward pass through the transformer model.\n\n    Parameters\n    ----------\n    input_ids\n        The input token ids.  Must be one or two dimensional.\n    attention_mask\n        The attention mask.  Must be one or two dimensional.\n    past_key_values\n        A tuple of tuples containing the cached key and value tensors for each\n        attention head.\n\n    Returns\n    -------\n    The computed logits and the new cached key and value tensors.\n\n    \"\"\"\n    assert 0 &lt; input_ids.ndim &lt; 3\n\n    if past_key_values:\n        input_ids = input_ids[..., -1].unsqueeze(-1)\n\n    output = self.model(\n        input_ids,\n        attention_mask=attention_mask,\n        return_dict=True,\n        output_attentions=False,\n        output_hidden_states=False,\n        past_key_values=past_key_values,\n    )\n    next_token_logits = output.logits[..., -1, :]\n\n    return next_token_logits, output.past_key_values\n</code></pre>"},{"location":"api/models/#outlines.models.transformers.TransformerTokenizer","title":"<code>TransformerTokenizer</code>","text":"<p>             Bases: <code>Tokenizer</code></p> <p>Represents a tokenizer for models in the <code>transformers</code> library.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>class TransformerTokenizer(Tokenizer):\n    \"\"\"Represents a tokenizer for models in the `transformers` library.\"\"\"\n\n    def __init__(self, model_name: str, **kwargs):\n        from transformers import AutoTokenizer\n\n        kwargs.setdefault(\"padding_side\", \"left\")\n        self.model_name = model_name\n        # TODO: Do something to make this hashable?\n        self.kwargs = kwargs\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name, **kwargs)\n        self.eos_token_id = self.tokenizer.eos_token_id\n        self.eos_token = self.tokenizer.eos_token\n\n        if not self.tokenizer.pad_token_id:\n            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n            self.pad_token_id = self.eos_token_id\n        else:\n            self.pad_token_id = self.tokenizer.pad_token_id\n            self.pad_token = self.tokenizer.pad_token\n\n        self.special_tokens = set(self.tokenizer.all_special_tokens)\n\n        self.vocabulary = self.tokenizer.get_vocab()\n        self.is_llama = isinstance(self.tokenizer, get_llama_tokenizer_types())\n\n    def encode(\n        self, prompt: Union[str, List[str]], **kwargs\n    ) -&gt; Tuple[torch.LongTensor, torch.LongTensor]:\n        kwargs[\"padding\"] = True\n        kwargs[\"return_tensors\"] = \"pt\"\n        output = self.tokenizer(prompt, **kwargs)\n        return output[\"input_ids\"], output[\"attention_mask\"]\n\n    def decode(self, token_ids: torch.LongTensor) -&gt; List[str]:\n        text = self.tokenizer.batch_decode(token_ids)\n        return text\n\n    def convert_token_to_string(self, token: str) -&gt; str:\n        from transformers.file_utils import SPIECE_UNDERLINE\n\n        string = self.tokenizer.convert_tokens_to_string([token])\n\n        if self.is_llama:\n            # A hack to handle missing spaces to HF's Llama tokenizers\n            if token.startswith(SPIECE_UNDERLINE) or token == \"&lt;0x20&gt;\":\n                return \" \" + string\n\n        return string\n\n    def __eq__(self, other):\n        if isinstance(other, type(self)):\n            return other.model_name == self.model_name and other.kwargs == self.kwargs\n        return NotImplemented\n\n    def __hash__(self):\n        from datasets.fingerprint import Hasher\n\n        return hash(Hasher.hash(self.tokenizer))\n</code></pre>"},{"location":"api/models/#outlines.models.transformers.get_llama_tokenizer_types","title":"<code>get_llama_tokenizer_types()</code>","text":"<p>Get all the Llama tokenizer types/classes that need work-arounds.</p> <p>When they can't be imported, a dummy class is created.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def get_llama_tokenizer_types():\n    \"\"\"Get all the Llama tokenizer types/classes that need work-arounds.\n\n    When they can't be imported, a dummy class is created.\n\n    \"\"\"\n    try:\n        from transformers.models.llama import LlamaTokenizer\n    except ImportError:\n\n        class LlamaTokenizer:  # type: ignore\n            pass\n\n    try:\n        from transformers.models.llama import LlamaTokenizerFast\n    except ImportError:\n\n        class LlamaTokenizerFast:  # type: ignore\n            pass\n\n    try:\n        from transformers.models.code_llama import CodeLlamaTokenizer\n    except ImportError:\n\n        class CodeLlamaTokenizer:  # type: ignore\n            pass\n\n    try:\n        from transformers.models.code_llama import CodeLlamaTokenizerFast\n    except ImportError:\n\n        class CodeLlamaTokenizerFast:  # type: ignore\n            pass\n\n    return (\n        LlamaTokenizer,\n        LlamaTokenizerFast,\n        CodeLlamaTokenizer,\n        CodeLlamaTokenizerFast,\n    )\n</code></pre>"},{"location":"api/models/#outlines.models.transformers.transformers","title":"<code>transformers(model_name, device=None, model_kwargs={}, tokenizer_kwargs={})</code>","text":"<p>Instantiate a model from the <code>transformers</code> library and its tokenizer.</p>"},{"location":"api/models/#outlines.models.transformers.transformers--parameters","title":"Parameters","text":"<p>model_name     The name of the model as listed on Hugging Face's model page. device     The device(s) on which the model should be loaded. This overrides     the <code>device_map</code> entry in <code>model_kwargs</code> when provided. model_kwargs     A dictionary that contains the keyword arguments to pass to the     <code>from_pretrained</code> method when loading the model. tokenizer_kwargs     A dictionary that contains the keyword arguments to pass to the     <code>from_pretrained</code> method when loading the tokenizer.</p>"},{"location":"api/models/#outlines.models.transformers.transformers--returns","title":"Returns","text":"<p>A <code>TransformersModel</code> model instance.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def transformers(\n    model_name: str,\n    device: Optional[str] = None,\n    model_kwargs: dict = {},\n    tokenizer_kwargs: dict = {},\n):\n    \"\"\"Instantiate a model from the `transformers` library and its tokenizer.\n\n    Parameters\n    ----------\n    model_name\n        The name of the model as listed on Hugging Face's model page.\n    device\n        The device(s) on which the model should be loaded. This overrides\n        the `device_map` entry in `model_kwargs` when provided.\n    model_kwargs\n        A dictionary that contains the keyword arguments to pass to the\n        `from_pretrained` method when loading the model.\n    tokenizer_kwargs\n        A dictionary that contains the keyword arguments to pass to the\n        `from_pretrained` method when loading the tokenizer.\n\n    Returns\n    -------\n    A `TransformersModel` model instance.\n\n    \"\"\"\n    try:\n        from transformers import AutoModelForCausalLM\n    except ImportError:\n        raise ImportError(\n            \"The `transformers` library needs to be installed in order to use `transformers` models.\"\n        )\n\n    if device is not None:\n        model_kwargs[\"device_map\"] = device\n\n    model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n    tokenizer = TransformerTokenizer(model_name, **tokenizer_kwargs)\n\n    return Transformer(model, tokenizer)\n</code></pre>"},{"location":"api/models/#outlines.models.openai.OpenAI","title":"<code>OpenAI</code>","text":"<p>An object that represents the OpenAI API.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>class OpenAI:\n    \"\"\"An object that represents the OpenAI API.\"\"\"\n\n    def __init__(\n        self,\n        model_name: str,\n        api_key: Optional[str] = None,\n        max_retries: int = 6,\n        config: Optional[OpenAIConfig] = None,\n    ):\n        \"\"\"Create an `OpenAI` instance.\n\n        Parameters\n        ----------\n        model_name\n            Model to use, as defined in OpenAI's documentation\n        api_key\n            Secret key to use with the OpenAI API. One can also set the\n            `OPENAI_API_KEY` environment variable, or the value of\n            `openai.api_key`.\n        max_retries\n            The maximum number of retries when calls to the API fail.\n        config\n            An instance of `OpenAIConfig`. Can be useful to specify some\n            parameters that cannot be set by calling this class' methods.\n\n        \"\"\"\n        try:\n            import openai\n        except ImportError:\n            raise ImportError(\n                \"The `openai` library needs to be installed in order to use Outlines' OpenAI integration.\"\n            )\n\n        if api_key is None:\n            if os.getenv(\"OPENAI_API_KEY\") is not None:\n                api_key = os.getenv(\"OPENAI_API_KEY\")\n            elif openai.api_key is not None:\n                api_key = openai.api_key\n            else:\n                raise ValueError(\n                    \"You must specify an API key to use the OpenAI API integration.\"\n                )\n\n        if config is not None:\n            self.config = replace(config, model=model_name)  # type: ignore\n        else:\n            self.config = OpenAIConfig(model=model_name)\n\n        self.client = openai.AsyncOpenAI(api_key=api_key, max_retries=max_retries)\n\n    def __call__(\n        self,\n        prompt: Union[str, List[str]],\n        max_tokens: Optional[int] = None,\n        *,\n        temperature: float = 1.0,\n        samples: int = 1,\n        stop_at: Optional[Union[List[str], str]] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"Call the OpenAI API to generate text.\n\n        Parameters\n        ----------\n        prompt\n            A string or list of strings that will be used to prompt the model\n        max_tokens\n            The maximum number of tokens to generate\n        temperature\n            The value of the temperature used to sample tokens\n        samples\n            The number of completions to generate for each prompt\n        stop_at\n            Up to 4 words where the API will stop the completion.\n\n        \"\"\"\n        config = replace(self.config, max_tokens=max_tokens, n=samples, stop=stop_at)  # type: ignore\n\n        if \"text-\" in self.config.model:\n            raise NotImplementedError(\n                textwrap.dedent(\n                    \"Most models that support the legacy completion endpoints will be \"\n                    \"deprecated on January 2024. Use Chat models instead.\\n\"\n                    \"The list of chat models is available at https://platform.openai.com/docs/guides/text-generation.\"\n                )\n            )\n        if \"gpt-\" in self.config.model:\n            return generate_chat(prompt, self.client, config)\n\n    def generate_choice(\n        self, prompt: str, choices: List[str], max_tokens: Optional[int] = None\n    ) -&gt; str:\n        \"\"\"Call the OpenAI API to generate one of several choices.\n\n        Parameters\n        ----------\n        prompt\n            A string or list of strings that will be used to prompt the model\n        choices\n            The list of strings between which we ask the model to choose\n        max_tokens\n            The maximum number of tokens to generate\n\n        \"\"\"\n        try:\n            import tiktoken\n        except ImportError:\n            raise ImportError(\n                \"The `tiktoken` library needs to be installed in order to choose `outlines.models.openai` with `is_in`\"\n            )\n\n        config = replace(self.config, max_tokens=max_tokens)\n\n        tokenizer = tiktoken.encoding_for_model(self.config.model)\n\n        greedy = False\n        decoded: List[str] = []\n        encoded_choices_left: List[List[int]] = [\n            tokenizer.encode(word) for word in choices\n        ]\n\n        while len(encoded_choices_left) &gt; 0:\n            max_tokens_left = max([len(tokens) for tokens in encoded_choices_left])\n            transposed_choices_left: List[Set] = [\n                {item for item in subset if item is not None}\n                for subset in zip_longest(*encoded_choices_left)\n            ]\n\n            if not greedy:\n                mask = build_optimistic_mask(transposed_choices_left)\n            else:\n                mask = {}\n                for token in transposed_choices_left[0]:  # build greedy mask\n                    mask[token] = 100\n\n            if len(mask) == 0:\n                break\n\n            config = replace(config, logit_bias=mask, max_tokens=max_tokens_left)\n            response = generate_chat(prompt, self.client, config)\n            encoded_response = tokenizer.encode(response)\n\n            if encoded_response in encoded_choices_left:\n                decoded.append(response)\n                break\n            else:\n                (\n                    encoded_response,\n                    encoded_choices_left,\n                ) = find_response_choices_intersection(\n                    encoded_response, encoded_choices_left\n                )\n\n                if len(encoded_response) == 0:\n                    greedy = True  # next iteration will be \"greedy\"\n                    continue\n                else:\n                    decoded.append(\"\".join(tokenizer.decode(encoded_response)))\n\n                    if len(encoded_choices_left) == 1:  # only one choice left\n                        choice_left = tokenizer.decode(encoded_choices_left[0])\n                        decoded.append(choice_left)\n                        break\n\n                    greedy = False  # after each success, stay with (or switch to) \"optimistic\" approach\n\n                prompt = prompt + \"\".join(decoded)\n\n        choice = \"\".join(decoded)\n\n        return choice\n\n    def generate_json(self):\n        \"\"\"Call the OpenAI API to generate a JSON object.\"\"\"\n        raise NotImplementedError\n\n    def __str__(self):\n        return self.__class__.__name__ + \" API\"\n\n    def __repr__(self):\n        return str(self.config)\n</code></pre>"},{"location":"api/models/#outlines.models.openai.OpenAI.__call__","title":"<code>__call__(prompt, max_tokens=None, *, temperature=1.0, samples=1, stop_at=None)</code>","text":"<p>Call the OpenAI API to generate text.</p>"},{"location":"api/models/#outlines.models.openai.OpenAI.__call__--parameters","title":"Parameters","text":"<p>prompt     A string or list of strings that will be used to prompt the model max_tokens     The maximum number of tokens to generate temperature     The value of the temperature used to sample tokens samples     The number of completions to generate for each prompt stop_at     Up to 4 words where the API will stop the completion.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def __call__(\n    self,\n    prompt: Union[str, List[str]],\n    max_tokens: Optional[int] = None,\n    *,\n    temperature: float = 1.0,\n    samples: int = 1,\n    stop_at: Optional[Union[List[str], str]] = None,\n) -&gt; np.ndarray:\n    \"\"\"Call the OpenAI API to generate text.\n\n    Parameters\n    ----------\n    prompt\n        A string or list of strings that will be used to prompt the model\n    max_tokens\n        The maximum number of tokens to generate\n    temperature\n        The value of the temperature used to sample tokens\n    samples\n        The number of completions to generate for each prompt\n    stop_at\n        Up to 4 words where the API will stop the completion.\n\n    \"\"\"\n    config = replace(self.config, max_tokens=max_tokens, n=samples, stop=stop_at)  # type: ignore\n\n    if \"text-\" in self.config.model:\n        raise NotImplementedError(\n            textwrap.dedent(\n                \"Most models that support the legacy completion endpoints will be \"\n                \"deprecated on January 2024. Use Chat models instead.\\n\"\n                \"The list of chat models is available at https://platform.openai.com/docs/guides/text-generation.\"\n            )\n        )\n    if \"gpt-\" in self.config.model:\n        return generate_chat(prompt, self.client, config)\n</code></pre>"},{"location":"api/models/#outlines.models.openai.OpenAI.__init__","title":"<code>__init__(model_name, api_key=None, max_retries=6, config=None)</code>","text":"<p>Create an <code>OpenAI</code> instance.</p>"},{"location":"api/models/#outlines.models.openai.OpenAI.__init__--parameters","title":"Parameters","text":"<p>model_name     Model to use, as defined in OpenAI's documentation api_key     Secret key to use with the OpenAI API. One can also set the     <code>OPENAI_API_KEY</code> environment variable, or the value of     <code>openai.api_key</code>. max_retries     The maximum number of retries when calls to the API fail. config     An instance of <code>OpenAIConfig</code>. Can be useful to specify some     parameters that cannot be set by calling this class' methods.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    api_key: Optional[str] = None,\n    max_retries: int = 6,\n    config: Optional[OpenAIConfig] = None,\n):\n    \"\"\"Create an `OpenAI` instance.\n\n    Parameters\n    ----------\n    model_name\n        Model to use, as defined in OpenAI's documentation\n    api_key\n        Secret key to use with the OpenAI API. One can also set the\n        `OPENAI_API_KEY` environment variable, or the value of\n        `openai.api_key`.\n    max_retries\n        The maximum number of retries when calls to the API fail.\n    config\n        An instance of `OpenAIConfig`. Can be useful to specify some\n        parameters that cannot be set by calling this class' methods.\n\n    \"\"\"\n    try:\n        import openai\n    except ImportError:\n        raise ImportError(\n            \"The `openai` library needs to be installed in order to use Outlines' OpenAI integration.\"\n        )\n\n    if api_key is None:\n        if os.getenv(\"OPENAI_API_KEY\") is not None:\n            api_key = os.getenv(\"OPENAI_API_KEY\")\n        elif openai.api_key is not None:\n            api_key = openai.api_key\n        else:\n            raise ValueError(\n                \"You must specify an API key to use the OpenAI API integration.\"\n            )\n\n    if config is not None:\n        self.config = replace(config, model=model_name)  # type: ignore\n    else:\n        self.config = OpenAIConfig(model=model_name)\n\n    self.client = openai.AsyncOpenAI(api_key=api_key, max_retries=max_retries)\n</code></pre>"},{"location":"api/models/#outlines.models.openai.OpenAI.generate_choice","title":"<code>generate_choice(prompt, choices, max_tokens=None)</code>","text":"<p>Call the OpenAI API to generate one of several choices.</p>"},{"location":"api/models/#outlines.models.openai.OpenAI.generate_choice--parameters","title":"Parameters","text":"<p>prompt     A string or list of strings that will be used to prompt the model choices     The list of strings between which we ask the model to choose max_tokens     The maximum number of tokens to generate</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def generate_choice(\n    self, prompt: str, choices: List[str], max_tokens: Optional[int] = None\n) -&gt; str:\n    \"\"\"Call the OpenAI API to generate one of several choices.\n\n    Parameters\n    ----------\n    prompt\n        A string or list of strings that will be used to prompt the model\n    choices\n        The list of strings between which we ask the model to choose\n    max_tokens\n        The maximum number of tokens to generate\n\n    \"\"\"\n    try:\n        import tiktoken\n    except ImportError:\n        raise ImportError(\n            \"The `tiktoken` library needs to be installed in order to choose `outlines.models.openai` with `is_in`\"\n        )\n\n    config = replace(self.config, max_tokens=max_tokens)\n\n    tokenizer = tiktoken.encoding_for_model(self.config.model)\n\n    greedy = False\n    decoded: List[str] = []\n    encoded_choices_left: List[List[int]] = [\n        tokenizer.encode(word) for word in choices\n    ]\n\n    while len(encoded_choices_left) &gt; 0:\n        max_tokens_left = max([len(tokens) for tokens in encoded_choices_left])\n        transposed_choices_left: List[Set] = [\n            {item for item in subset if item is not None}\n            for subset in zip_longest(*encoded_choices_left)\n        ]\n\n        if not greedy:\n            mask = build_optimistic_mask(transposed_choices_left)\n        else:\n            mask = {}\n            for token in transposed_choices_left[0]:  # build greedy mask\n                mask[token] = 100\n\n        if len(mask) == 0:\n            break\n\n        config = replace(config, logit_bias=mask, max_tokens=max_tokens_left)\n        response = generate_chat(prompt, self.client, config)\n        encoded_response = tokenizer.encode(response)\n\n        if encoded_response in encoded_choices_left:\n            decoded.append(response)\n            break\n        else:\n            (\n                encoded_response,\n                encoded_choices_left,\n            ) = find_response_choices_intersection(\n                encoded_response, encoded_choices_left\n            )\n\n            if len(encoded_response) == 0:\n                greedy = True  # next iteration will be \"greedy\"\n                continue\n            else:\n                decoded.append(\"\".join(tokenizer.decode(encoded_response)))\n\n                if len(encoded_choices_left) == 1:  # only one choice left\n                    choice_left = tokenizer.decode(encoded_choices_left[0])\n                    decoded.append(choice_left)\n                    break\n\n                greedy = False  # after each success, stay with (or switch to) \"optimistic\" approach\n\n            prompt = prompt + \"\".join(decoded)\n\n    choice = \"\".join(decoded)\n\n    return choice\n</code></pre>"},{"location":"api/models/#outlines.models.openai.OpenAI.generate_json","title":"<code>generate_json()</code>","text":"<p>Call the OpenAI API to generate a JSON object.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def generate_json(self):\n    \"\"\"Call the OpenAI API to generate a JSON object.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/models/#outlines.models.openai.OpenAIConfig","title":"<code>OpenAIConfig</code>  <code>dataclass</code>","text":"<p>Represents the parameters of the OpenAI API.</p> <p>The information was last fetched on 2023/11/20. We document below the properties that are specific to the OpenAI API. Not all these properties are supported by Outlines.</p>"},{"location":"api/models/#outlines.models.openai.OpenAIConfig--properties","title":"Properties","text":"<p>model_name     The name of the model. Available models can be found on OpenAI's website. frequence_penalty     Number between 2.0 and -2.0. Positive values penalize new tokens based on     their existing frequency in the text, logit_bias     Modifies the likelihood of specified tokens to appear in the completion.     Number between -100 (forbid) and +100 (only allows). n     The number of completions to return for each prompt. presence_penalty     Similar to frequency penalty. response_format     Specifies the format the model must output. <code>{\"type\": \"json_object\"}</code>     enables JSON mode. seed     Two completions with the same <code>seed</code> value should return the same     completion. This is however not guaranteed. stop     Up to 4 words where the API will stop the completion. temperature     Number between 0 and 2. Higher values make the output more random, while     lower values make it more deterministic. top_p     Number between 0 and 1. Parameter for nucleus sampling. user     A unique identifier for the end-user.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>@dataclass(frozen=True)\nclass OpenAIConfig:\n    \"\"\"Represents the parameters of the OpenAI API.\n\n    The information was last fetched on 2023/11/20. We document below the\n    properties that are specific to the OpenAI API. Not all these properties are\n    supported by Outlines.\n\n    Properties\n    ----------\n    model_name\n        The name of the model. Available models can be found on OpenAI's website.\n    frequence_penalty\n        Number between 2.0 and -2.0. Positive values penalize new tokens based on\n        their existing frequency in the text,\n    logit_bias\n        Modifies the likelihood of specified tokens to appear in the completion.\n        Number between -100 (forbid) and +100 (only allows).\n    n\n        The number of completions to return for each prompt.\n    presence_penalty\n        Similar to frequency penalty.\n    response_format\n        Specifies the format the model must output. `{\"type\": \"json_object\"}`\n        enables JSON mode.\n    seed\n        Two completions with the same `seed` value should return the same\n        completion. This is however not guaranteed.\n    stop\n        Up to 4 words where the API will stop the completion.\n    temperature\n        Number between 0 and 2. Higher values make the output more random, while\n        lower values make it more deterministic.\n    top_p\n        Number between 0 and 1. Parameter for nucleus sampling.\n    user\n        A unique identifier for the end-user.\n\n    \"\"\"\n\n    model: str\n    frequency_penalty: float = 0\n    logit_bias: Dict[int, int] = field(default_factory=dict)\n    max_tokens: Optional[int] = None\n    n: int = 1\n    presence_penalty: float = 0\n    response_format: Optional[Dict[str, str]] = None\n    seed: Optional[int] = None\n    stop: Optional[Union[str, List[str]]] = None\n    temperature: Optional[float] = None\n    top_p: int = 1\n    user: str = field(default_factory=str)\n</code></pre>"},{"location":"api/models/#outlines.models.openai.build_optimistic_mask","title":"<code>build_optimistic_mask(transposed, max_mask_size=300)</code>","text":"<p>We build the largest mask possible.</p> <p>Tokens are added from left to right, so if the encoded choices are e.g. <code>[[1,2], [3,4]]</code>, <code>1</code> and <code>3</code> will be added before <code>2</code> and <code>4</code>.</p>"},{"location":"api/models/#outlines.models.openai.build_optimistic_mask--parameters","title":"Parameters","text":"<p>transposed     A list of lists that contain the nth token of each choice.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def build_optimistic_mask(\n    transposed: List[Set[int]], max_mask_size: int = 300\n) -&gt; Dict[int, int]:\n    \"\"\"We build the largest mask possible.\n\n    Tokens are added from left to right, so if the encoded choices are e.g.\n    `[[1,2], [3,4]]`, `1` and `3` will be added before `2` and `4`.\n\n    Parameters\n    ----------\n    transposed\n        A list of lists that contain the nth token of each choice.\n\n    \"\"\"\n    mask: Dict[int, int] = {}\n    for tokens in transposed:\n        for token in tokens:\n            if len(mask) == max_mask_size:\n                return mask\n            mask[token] = 100\n\n    return mask\n</code></pre>"},{"location":"api/models/#outlines.models.openai.error_handler","title":"<code>error_handler(api_call_fn)</code>","text":"<p>Handle OpenAI API errors and missing API key.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def error_handler(api_call_fn: Callable) -&gt; Callable:\n    \"\"\"Handle OpenAI API errors and missing API key.\"\"\"\n\n    def call(*args, **kwargs):\n        import openai\n\n        try:\n            return api_call_fn(*args, **kwargs)\n        except (\n            openai.APITimeoutError,\n            openai.InternalServerError,\n            openai.RateLimitError,\n        ) as e:\n            raise OSError(f\"Could not connect to the OpenAI API: {e}\")\n        except (\n            openai.AuthenticationError,\n            openai.BadRequestError,\n            openai.ConflictError,\n            openai.PermissionDeniedError,\n            openai.NotFoundError,\n            openai.UnprocessableEntityError,\n        ) as e:\n            raise e\n\n    return call\n</code></pre>"},{"location":"api/models/#outlines.models.openai.find_longest_intersection","title":"<code>find_longest_intersection(response, choice)</code>","text":"<p>Find the longest intersection between the response and the choice.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def find_longest_intersection(response: List[int], choice: List[int]) -&gt; List[int]:\n    \"\"\"Find the longest intersection between the response and the choice.\"\"\"\n    for i, (token_r, token_c) in enumerate(zip_longest(response, choice)):\n        if token_r != token_c:\n            return response[:i]\n\n    return response\n</code></pre>"},{"location":"api/models/#outlines.models.openai.find_response_choices_intersection","title":"<code>find_response_choices_intersection(response, choices)</code>","text":"<p>Find the longest intersection between the response and the different choices.</p> <p>Say the response is of the form <code>[1, 2, 3, 4, 5]</code> and we have the choices <code>[[1, 2], [1, 2, 3], [6, 7, 8]</code> then the function will return <code>[1, 2]</code> as the intersection, and <code>[1, 2, 3]</code> as the choice that is left.</p>"},{"location":"api/models/#outlines.models.openai.find_response_choices_intersection--parameters","title":"Parameters","text":"<p>response     The model's response choices     The remaining possible choices</p>"},{"location":"api/models/#outlines.models.openai.find_response_choices_intersection--returns","title":"Returns","text":"<p>A tuple that contains the longest intersection between the response and the different choices, and the choices which start with this intersection.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def find_response_choices_intersection(\n    response: List[int], choices: List[List[int]]\n) -&gt; Tuple[List[int], List[List[int]]]:\n    \"\"\"Find the longest intersection between the response and the different\n    choices.\n\n    Say the response is of the form `[1, 2, 3, 4, 5]` and we have the choices\n    `[[1, 2], [1, 2, 3], [6, 7, 8]` then the function will return `[1, 2]` as the\n    intersection, and `[1, 2, 3]` as the choice that is left.\n\n    Parameters\n    ----------\n    response\n        The model's response\n    choices\n        The remaining possible choices\n\n    Returns\n    -------\n    A tuple that contains the longest intersection between the response and the\n    different choices, and the choices which start with this intersection.\n\n    \"\"\"\n    max_len_prefix = 0\n    choices_left = []\n    longest_prefix = []\n    for i, choice in enumerate(choices):\n        # Find the longest intersection between the response and the choice.\n        prefix = find_longest_intersection(response, choice)\n\n        if len(prefix) &gt; max_len_prefix:\n            max_len_prefix = len(prefix)\n            choices_left = [choice[len(prefix) :]]\n            longest_prefix = prefix\n\n        elif len(prefix) == max_len_prefix:\n            choices_left.append(choice[len(prefix) :])\n\n    return longest_prefix, choices_left\n</code></pre>"},{"location":"api/parsing/","title":"Parsing","text":""},{"location":"api/parsing/#outlines.text.parsing.PartialIndenter","title":"<code>PartialIndenter</code>","text":"<p>             Bases: <code>Indenter</code></p> <p>An <code>Indenter</code> that doesn't reset its state every time <code>process</code> is called.</p> Source code in <code>outlines/text/parsing.py</code> <pre><code>class PartialIndenter(Indenter):\n    \"\"\"An `Indenter` that doesn't reset its state every time `process` is called.\"\"\"\n\n    def process(self, stream):\n        return self._process(stream)\n\n    def _process(self, stream):\n        for token in stream:\n            # These were previously *after* the `yield`, but that makes the\n            # state tracking unnecessarily convoluted.\n            if token.type in self.OPEN_PAREN_types:\n                self.paren_level += 1\n            elif token.type in self.CLOSE_PAREN_types:\n                self.paren_level -= 1\n                if self.paren_level &lt; 0:\n                    raise UnexpectedToken(token, [])\n\n            if token.type == self.NL_type:\n                yield from self.handle_NL(token)\n            else:\n                yield token\n\n        # TODO: What do we want to do here?\n        # while len(self.indent_level) &gt; 1:\n        #     self.indent_level.pop()\n        #     yield Token(self.DEDENT_type, \"\")\n\n    def accepts_token_type(self, token_type):\n        if token_type in self.CLOSE_PAREN_types and self.paren_level - 1 &lt; 0:\n            return False\n\n        # TODO:\n        # if token_type == self.NL_type and self.paren_level == 0:\n        #     ...\n        #     return False\n\n        return True\n\n    def __copy__(self):\n        res = type(self)()\n        res.paren_level = self.paren_level\n        res.indent_level = copy(self.indent_level)\n        return res\n\n    def __repr__(self):\n        return f\"{type(self).__name__}(paren_level={self.paren_level!r}, indent_level={self.indent_level!r})\"\n</code></pre>"},{"location":"api/parsing/#outlines.text.parsing.PartialParserState","title":"<code>PartialParserState</code>","text":"<p>             Bases: <code>ParserState</code></p> Source code in <code>outlines/text/parsing.py</code> <pre><code>class PartialParserState(ParserState):\n    __slots__ = \"use_value_stack\"\n\n    def __init__(\n        self,\n        parse_conf,\n        lexer,\n        state_stack=None,\n        value_stack=None,\n        use_value_stack=False,\n    ):\n        super().__init__(\n            parse_conf, lexer, state_stack=state_stack, value_stack=value_stack\n        )\n        self.use_value_stack = use_value_stack\n\n    def feed_token(self, token, is_end=False):\n        if token.type == \"partial\":\n            # If none of the potential terminals can transition, we need to know now\n            current_state = self.state_stack[-1]\n            current_lexer = get_contextual_lexer(self.lexer).lexers[current_state]\n\n            # We have to feed the token and determine whether or not at least\n            # one terminal is consistent with the stack; otherwise, we'll miss\n            # invalid REDUCE cases.\n            # TODO: We should track separate parses conditional on possible\n            # token/symbol types, then we can coherently reuse the following\n            # results instead of recomputing it later.\n            can_transition = False\n            for terminal_info in token.value.terminals_and_info:\n                if terminal_info.terminal_name not in current_lexer.ignore_types:\n                    test_token = Token.new_borrow_pos(\n                        terminal_info.terminal_name, \"\", token\n                    )\n\n                    stack = copy(self.state_stack)\n                    try:\n                        self.feed_token_no_stack(test_token, is_end=is_end)\n                        can_transition = True\n                        break\n                    except UnexpectedToken:\n                        continue\n                    finally:\n                        self.state_stack = stack\n                else:\n                    can_transition = True\n\n            if not can_transition:\n                expected = {\n                    s\n                    for s in self.parse_conf.states[current_state].keys()\n                    if s.isupper()\n                }\n                raise UnexpectedToken(\n                    token, expected, state=self, interactive_parser=None\n                )\n\n        elif self.use_value_stack:\n            super().feed_token(token, is_end=is_end)\n        else:\n            self.feed_token_no_stack(token, is_end=is_end)\n\n    def feed_token_no_stack(self, token, is_end=False):\n        \"\"\"\n        This is a copy of `ParserState.feed_token` with all the value stack\n        steps removed.  Since we're not exactly parsing in order to obtain a\n        CST or anything similar, we can avoid the growing expense of tracking\n        the parse tree.\n        \"\"\"\n        state_stack = self.state_stack\n        states = self.parse_conf.states\n        end_state = self.parse_conf.end_state\n\n        while True:\n            state = state_stack[-1]\n            try:\n                action, arg = states[state][token.type]\n            except KeyError:\n                expected = {s for s in states[state].keys() if s.isupper()}\n                raise UnexpectedToken(\n                    token, expected, state=self, interactive_parser=None\n                )\n\n            assert arg != end_state\n\n            if action is Shift:\n                # shift once and return\n                assert not is_end\n                state_stack.append(arg)\n                return\n            else:\n                # reduce+shift as many times as necessary\n                rule = arg\n                size = len(rule.expansion)\n                if size:\n                    del state_stack[-size:]\n\n                _action, new_state = states[state_stack[-1]][rule.origin.name]\n                assert _action is Shift\n                state_stack.append(new_state)\n\n                if is_end and state_stack[-1] == end_state:\n                    return\n\n    def __copy__(self):\n        return type(self)(\n            self.parse_conf,\n            copy(self.lexer),\n            copy(self.state_stack),\n            deepcopy(self.value_stack),\n            use_value_stack=self.use_value_stack,\n        )\n\n    def __repr__(self):\n        return f\"{type(self).__name__}(lexer={self.lexer!r}, state_stack={self.state_stack!r})\"\n</code></pre>"},{"location":"api/parsing/#outlines.text.parsing.PartialParserState.feed_token_no_stack","title":"<code>feed_token_no_stack(token, is_end=False)</code>","text":"<p>This is a copy of <code>ParserState.feed_token</code> with all the value stack steps removed.  Since we're not exactly parsing in order to obtain a CST or anything similar, we can avoid the growing expense of tracking the parse tree.</p> Source code in <code>outlines/text/parsing.py</code> <pre><code>def feed_token_no_stack(self, token, is_end=False):\n    \"\"\"\n    This is a copy of `ParserState.feed_token` with all the value stack\n    steps removed.  Since we're not exactly parsing in order to obtain a\n    CST or anything similar, we can avoid the growing expense of tracking\n    the parse tree.\n    \"\"\"\n    state_stack = self.state_stack\n    states = self.parse_conf.states\n    end_state = self.parse_conf.end_state\n\n    while True:\n        state = state_stack[-1]\n        try:\n            action, arg = states[state][token.type]\n        except KeyError:\n            expected = {s for s in states[state].keys() if s.isupper()}\n            raise UnexpectedToken(\n                token, expected, state=self, interactive_parser=None\n            )\n\n        assert arg != end_state\n\n        if action is Shift:\n            # shift once and return\n            assert not is_end\n            state_stack.append(arg)\n            return\n        else:\n            # reduce+shift as many times as necessary\n            rule = arg\n            size = len(rule.expansion)\n            if size:\n                del state_stack[-size:]\n\n            _action, new_state = states[state_stack[-1]][rule.origin.name]\n            assert _action is Shift\n            state_stack.append(new_state)\n\n            if is_end and state_stack[-1] == end_state:\n                return\n</code></pre>"},{"location":"api/parsing/#outlines.text.parsing.PartialParsingFrontend","title":"<code>PartialParsingFrontend</code>","text":"<p>             Bases: <code>ParsingFrontend</code></p> Source code in <code>outlines/text/parsing.py</code> <pre><code>class PartialParsingFrontend(ParsingFrontend):\n    def __init__(self, lexer_conf, parser_conf, options, parser=None):\n        assert parser_conf.parser_type == \"lalr\"\n\n        options._plugins[\"LALR_Parser\"] = PartialLALRParser\n        options._plugins[\"BasicLexer\"] = PartialBasicLexer\n        options._plugins[\"ContextualLexer\"] = PartialContextualLexer\n        options._plugins[\"LexerThread\"] = PartialLexerThread\n\n        super().__init__(lexer_conf, parser_conf, options, parser=parser)\n\n        if lexer_conf.postlex:\n            self.lexer = PartialPostLexConnector(self.lexer.lexer, lexer_conf.postlex)\n\n        self._termset_fsm_info = None\n        self._symbols_to_states: Optional[\n            Dict[str, Set[Tuple[ParseStateType, Action]]]\n        ] = None\n        self._reverse_shifts: Optional[\n            Dict[ParseStateType, Dict[str, Set[ParseStateType]]]\n        ] = None\n        # self._state_transition_map: Optional[\n        #     Dict[Tuple[ParseStateType, str], Set[ParseStateType]]\n        # ] = None\n\n    def _compute_maps(\n        self,\n    ):\n        \"\"\"Compute state transition and symbols-to-states maps.\"\"\"\n        self._reverse_shifts = {}\n        self._symbols_to_states = {}\n\n        parse_table = self.parser.parser.parse_table\n\n        for from_state, symbols_to_ops in parse_table.states.items():\n            for symbol, op in symbols_to_ops.items():\n                if op[0] == Shift:\n                    symbols_to_from_states = self._reverse_shifts.setdefault(op[1], {})\n                    symbols_to_from_states.setdefault(symbol, set()).add(from_state)\n                self._symbols_to_states.setdefault(symbol, set()).add((from_state, op))\n\n        # # TODO: This approach is very wasteful.\n        # context_lexer = get_contextual_lexer(self)\n        # self._state_transition_map = {}\n        #\n        # for from_state, transitions in parse_table.states.items():\n        #     for symbol, action in transitions.items():\n        #         # TODO: Filter non-terminals\n        #         if symbol not in context_lexer.root_lexer.terminals_by_name:\n        #             continue\n        #\n        #         if action[0] is Shift:\n        #             self._state_transition_map.setdefault(\n        #                 (from_state, symbol), set()\n        #             ).add(action[1])\n        #             continue\n        #\n        #         antecedent_state_seqs = parse_to_terminal(self, [(from_state,)], symbol)\n        #\n        #         for antecedent_state_seq in antecedent_state_seqs:\n        #             antecedent_state = antecedent_state_seq[-1]\n        #             self._state_transition_map.setdefault(\n        #                 (from_state, symbol), set()\n        #             ).add(antecedent_state)\n\n    def _compute_termset_fsm_info(self):\n        \"\"\"Collect and return information about terminal symbol sets and their FSMs.\n\n        Terminal symbol sets (or \"termsets\") are ordered sequences of terminal\n        symbols that are used by each parser state.  Associated with each is a\n        collection of FSMs for each terminal and a single parse state FSM that is\n        the union of each terminal's FSM.\n\n        This constructs a list of tuples containing the termset, the set of\n        parse states that use the termsets, parse state FSMs, and information\n        mapping the components of the parse state FSMs to their terminal symbol\n        FSMs.\n\n        \"\"\"\n        context_lexer = get_contextual_lexer(self)\n        termsets_to_fsms = {}\n        termsets_to_parse_states: Dict[Tuple[str, ...], Set[ParseStateType]] = {}\n        for parse_state, lexer in context_lexer.lexers.items():\n            scanner = lexer.scanner\n            key = tuple(term.name for term in scanner.terminals)\n            termsets_to_fsms[key] = (scanner.fsm, scanner.fsms_to_trans_finals)\n            termsets_to_parse_states.setdefault(key, set()).add(parse_state)\n\n        self._termset_fsm_info = [\n            (\n                termset,\n                frozenset(termsets_to_parse_states[termset]),\n                fsm,\n                fsms_to_trans_finals,\n            )\n            for termset, (fsm, fsms_to_trans_finals) in termsets_to_fsms.items()\n        ]\n\n    @property\n    def termset_fsm_info(self):\n        if self._termset_fsm_info is None:\n            self._compute_termset_fsm_info()\n        return self._termset_fsm_info\n\n    @property\n    def symbols_to_states(self):\n        if self._symbols_to_states is None:\n            self._compute_maps()\n        return self._symbols_to_states\n\n    @property\n    def reverse_shifts(self):\n        if self._reverse_shifts is None:\n            self._compute_maps()\n        return self._reverse_shifts\n</code></pre>"},{"location":"api/parsing/#outlines.text.parsing.PartialScanner","title":"<code>PartialScanner</code>","text":"<p>             Bases: <code>Scanner</code></p> Source code in <code>outlines/text/parsing.py</code> <pre><code>class PartialScanner(Scanner):\n    @classmethod\n    @lru_cache\n    def construct_terminal_fsm(cls, terminal):\n        # TODO: This should really be done at the lexer/parser level so that\n        # the lifetime of these objects is tied to the parser itself.\n        regex_str = terminal.pattern.to_regexp()\n        pattern = interegular.parse_pattern(regex_str)\n        fsm, _ = make_deterministic_fsm(pattern.to_fsm().reduce())\n        return fsm, pattern.prefix_postfix\n\n    def __init__(self, terminals, g_regex_flags, re_, use_bytes, match_whole=False):\n        self.terminals = terminals\n        self.g_regex_flags = g_regex_flags\n        self.use_bytes = use_bytes\n        self.match_whole = match_whole\n        self.allowed_types = {t.name for t in self.terminals}\n        self._mres = None\n\n        fsms = []\n        for t in self.terminals:\n            fsm, prefix_postfix = self.construct_terminal_fsm(t)\n\n            # TODO FIXME: We don't support this right now.\n            assert prefix_postfix == (0, 0)\n\n            fsms.append(fsm)\n\n        self.fsm, self.fsms_to_trans_finals = fsm_union(fsms)\n\n    def get_terminals_info(\n        self, fsm_state_seq\n    ) -&gt; Tuple[Tuple[PartialTerminalInfo, ...], Tuple[PartialTerminalInfo, ...]]:\n        \"\"\"Get the possible terminal symbols for an FSM state sequence.\"\"\"\n        terminals_and_info: Tuple[PartialTerminalInfo, ...] = ()\n        final_terminals_and_info: Tuple[PartialTerminalInfo, ...] = ()\n        for i, (fsm_id, fsm_reads_more, in_final) in enumerate(\n            get_sub_fsms_from_seq(fsm_state_seq, self.fsms_to_trans_finals)\n        ):\n            terminal_name = self.terminals[fsm_id].name\n            info = PartialTerminalInfo(i, terminal_name, fsm_reads_more, in_final)\n            terminals_and_info += (info,)\n            if in_final:\n                final_terminals_and_info += (info,)\n\n        return terminals_and_info, final_terminals_and_info\n\n    def match(self, text, pos, last_fsm_state_seq: Optional[Tuple[int, ...]] = None):\n        \"\"\"Determine an FSM match over `text` starting at `pos` and continuing `last_fsm_state_seq`.\"\"\"\n\n        start_pos = pos\n\n        if last_fsm_state_seq:\n            assert len(last_fsm_state_seq) &gt; 1\n            start_pos += len(last_fsm_state_seq) - 1\n            start_state = last_fsm_state_seq[-1]\n        else:\n            start_state = self.fsm.initial\n\n        text_part = text[start_pos:]\n\n        state_seq = walk_fsm(\n            self.fsm,\n            text_part,\n            start_state,\n            full_match=self.match_whole,\n        )\n\n        if not state_seq:\n            return None\n\n        if last_fsm_state_seq:\n            res = last_fsm_state_seq + tuple(state_seq)\n        else:\n            res = (start_state,) + tuple(state_seq)\n\n        return res\n</code></pre>"},{"location":"api/parsing/#outlines.text.parsing.PartialScanner.get_terminals_info","title":"<code>get_terminals_info(fsm_state_seq)</code>","text":"<p>Get the possible terminal symbols for an FSM state sequence.</p> Source code in <code>outlines/text/parsing.py</code> <pre><code>def get_terminals_info(\n    self, fsm_state_seq\n) -&gt; Tuple[Tuple[PartialTerminalInfo, ...], Tuple[PartialTerminalInfo, ...]]:\n    \"\"\"Get the possible terminal symbols for an FSM state sequence.\"\"\"\n    terminals_and_info: Tuple[PartialTerminalInfo, ...] = ()\n    final_terminals_and_info: Tuple[PartialTerminalInfo, ...] = ()\n    for i, (fsm_id, fsm_reads_more, in_final) in enumerate(\n        get_sub_fsms_from_seq(fsm_state_seq, self.fsms_to_trans_finals)\n    ):\n        terminal_name = self.terminals[fsm_id].name\n        info = PartialTerminalInfo(i, terminal_name, fsm_reads_more, in_final)\n        terminals_and_info += (info,)\n        if in_final:\n            final_terminals_and_info += (info,)\n\n    return terminals_and_info, final_terminals_and_info\n</code></pre>"},{"location":"api/parsing/#outlines.text.parsing.PartialScanner.match","title":"<code>match(text, pos, last_fsm_state_seq=None)</code>","text":"<p>Determine an FSM match over <code>text</code> starting at <code>pos</code> and continuing <code>last_fsm_state_seq</code>.</p> Source code in <code>outlines/text/parsing.py</code> <pre><code>def match(self, text, pos, last_fsm_state_seq: Optional[Tuple[int, ...]] = None):\n    \"\"\"Determine an FSM match over `text` starting at `pos` and continuing `last_fsm_state_seq`.\"\"\"\n\n    start_pos = pos\n\n    if last_fsm_state_seq:\n        assert len(last_fsm_state_seq) &gt; 1\n        start_pos += len(last_fsm_state_seq) - 1\n        start_state = last_fsm_state_seq[-1]\n    else:\n        start_state = self.fsm.initial\n\n    text_part = text[start_pos:]\n\n    state_seq = walk_fsm(\n        self.fsm,\n        text_part,\n        start_state,\n        full_match=self.match_whole,\n    )\n\n    if not state_seq:\n        return None\n\n    if last_fsm_state_seq:\n        res = last_fsm_state_seq + tuple(state_seq)\n    else:\n        res = (start_state,) + tuple(state_seq)\n\n    return res\n</code></pre>"},{"location":"api/parsing/#outlines.text.parsing.terminals_to_fsms","title":"<code>terminals_to_fsms(lp)</code>","text":"<p>Construct a <code>dict</code> mapping terminal symbol names to their finite state machines.</p> Source code in <code>outlines/text/parsing.py</code> <pre><code>def terminals_to_fsms(lp: PartialLark) -&gt; Dict[str, FSM]:\n    \"\"\"Construct a ``dict`` mapping terminal symbol names to their finite state machines.\"\"\"\n\n    symbol_names_and_fsms = {}\n    for terminal in lp.terminals:\n        pattern = interegular.parse_pattern(terminal.pattern.to_regexp())\n        # TODO: Use `pyparser.terminals[0].pattern.flags`?\n        try:\n            fsm, _ = make_deterministic_fsm(pattern.to_fsm().reduce())\n        except Unsupported:\n            fsm = None\n\n        symbol_names_and_fsms[terminal.name] = fsm\n\n    return symbol_names_and_fsms\n</code></pre>"},{"location":"api/prompts/","title":"Prompts","text":""},{"location":"api/prompts/#outlines.text.prompts.Prompt","title":"<code>Prompt</code>  <code>dataclass</code>","text":"<p>Represents a prompt function.</p> <p>We return a <code>Prompt</code> class instead of a simple function so the template defined in prompt functions can be accessed.</p> Source code in <code>outlines/text/prompts.py</code> <pre><code>@dataclass\nclass Prompt:\n    \"\"\"Represents a prompt function.\n\n    We return a `Prompt` class instead of a simple function so the\n    template defined in prompt functions can be accessed.\n\n    \"\"\"\n\n    template: str\n    signature: inspect.Signature\n\n    def __post_init__(self):\n        self.parameters: List[str] = list(self.signature.parameters.keys())\n\n    def __call__(self, *args, **kwargs) -&gt; str:\n        \"\"\"Render and return the template.\n\n        Returns\n        -------\n        The rendered template as a Python ``str``.\n\n        \"\"\"\n        bound_arguments = self.signature.bind(*args, **kwargs)\n        bound_arguments.apply_defaults()\n        return render(self.template, **bound_arguments.arguments)\n\n    def __str__(self):\n        return self.template\n</code></pre>"},{"location":"api/prompts/#outlines.text.prompts.Prompt.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Render and return the template.</p>"},{"location":"api/prompts/#outlines.text.prompts.Prompt.__call__--returns","title":"Returns","text":"<p>The rendered template as a Python <code>str</code>.</p> Source code in <code>outlines/text/prompts.py</code> <pre><code>def __call__(self, *args, **kwargs) -&gt; str:\n    \"\"\"Render and return the template.\n\n    Returns\n    -------\n    The rendered template as a Python ``str``.\n\n    \"\"\"\n    bound_arguments = self.signature.bind(*args, **kwargs)\n    bound_arguments.apply_defaults()\n    return render(self.template, **bound_arguments.arguments)\n</code></pre>"},{"location":"api/prompts/#outlines.text.prompts.get_fn_description","title":"<code>get_fn_description(fn)</code>","text":"<p>Returns the first line of a callable's docstring.</p> Source code in <code>outlines/text/prompts.py</code> <pre><code>def get_fn_description(fn: Callable):\n    \"\"\"Returns the first line of a callable's docstring.\"\"\"\n    if not callable(fn):\n        raise TypeError(\"The `description` filter only applies to callables.\")\n\n    docstring = inspect.getdoc(fn)\n    if docstring is None:\n        description = \"\"\n    else:\n        description = docstring.split(\"\\n\")[0].strip()\n\n    return description\n</code></pre>"},{"location":"api/prompts/#outlines.text.prompts.get_fn_name","title":"<code>get_fn_name(fn)</code>","text":"<p>Returns the name of a callable.</p> Source code in <code>outlines/text/prompts.py</code> <pre><code>def get_fn_name(fn: Callable):\n    \"\"\"Returns the name of a callable.\"\"\"\n    if not callable(fn):\n        raise TypeError(\"The `name` filter only applies to callables.\")\n\n    if not hasattr(fn, \"__name__\"):\n        name = type(fn).__name__\n    else:\n        name = fn.__name__\n\n    return name\n</code></pre>"},{"location":"api/prompts/#outlines.text.prompts.get_fn_signature","title":"<code>get_fn_signature(fn)</code>","text":"<p>Return the signature of a callable.</p> Source code in <code>outlines/text/prompts.py</code> <pre><code>def get_fn_signature(fn: Callable):\n    \"\"\"Return the signature of a callable.\"\"\"\n    if not callable(fn):\n        raise TypeError(\"The `source` filter only applies to callables.\")\n\n    source = textwrap.dedent(inspect.getsource(fn))\n    re_search = re.search(re.compile(r\"\\(([^)]+)\\)\"), source)\n    if re_search is None:\n        signature = \"\"\n    else:\n        signature = re_search.group(1)\n\n    return signature\n</code></pre>"},{"location":"api/prompts/#outlines.text.prompts.get_fn_source","title":"<code>get_fn_source(fn)</code>","text":"<p>Return the source code of a callable.</p> Source code in <code>outlines/text/prompts.py</code> <pre><code>def get_fn_source(fn: Callable):\n    \"\"\"Return the source code of a callable.\"\"\"\n    if not callable(fn):\n        raise TypeError(\"The `source` filter only applies to callables.\")\n\n    source = textwrap.dedent(inspect.getsource(fn))\n    re_search = re.search(re.compile(r\"(\\bdef\\b.*)\", re.DOTALL), source)\n    if re_search is not None:\n        source = re_search.group(0)\n    else:\n        raise TypeError(\"Could not read the function's source code\")\n\n    return source\n</code></pre>"},{"location":"api/prompts/#outlines.text.prompts.get_schema_dict","title":"<code>get_schema_dict(model)</code>","text":"<p>Return a pretty-printed dictionary</p> Source code in <code>outlines/text/prompts.py</code> <pre><code>@get_schema.register(dict)\ndef get_schema_dict(model: Dict):\n    \"\"\"Return a pretty-printed dictionary\"\"\"\n    return json.dumps(model, indent=2)\n</code></pre>"},{"location":"api/prompts/#outlines.text.prompts.get_schema_pydantic","title":"<code>get_schema_pydantic(model)</code>","text":"<p>Return the schema of a Pydantic model.</p> Source code in <code>outlines/text/prompts.py</code> <pre><code>@get_schema.register(type(BaseModel))\ndef get_schema_pydantic(model: Type[BaseModel]):\n    \"\"\"Return the schema of a Pydantic model.\"\"\"\n    if not type(model) == type(BaseModel):\n        raise TypeError(\"The `schema` filter only applies to Pydantic models.\")\n\n    if hasattr(model, \"model_json_schema\"):\n        def_key = \"$defs\"\n        raw_schema = model.model_json_schema()\n    else:  # pragma: no cover\n        def_key = \"definitions\"\n        raw_schema = model.schema()\n\n    definitions = raw_schema.get(def_key, None)\n    schema = parse_pydantic_schema(raw_schema, definitions)\n\n    return json.dumps(schema, indent=2)\n</code></pre>"},{"location":"api/prompts/#outlines.text.prompts.parse_pydantic_schema","title":"<code>parse_pydantic_schema(raw_schema, definitions)</code>","text":"<p>Parse the output of <code>Basemodel.[schema|model_json_schema]()</code>.</p> <p>This recursively follows the references to other schemas in case of nested models. Other schemas are stored under the \"definitions\" key in the schema of the top-level model.</p> Source code in <code>outlines/text/prompts.py</code> <pre><code>def parse_pydantic_schema(raw_schema, definitions):\n    \"\"\"Parse the output of `Basemodel.[schema|model_json_schema]()`.\n\n    This recursively follows the references to other schemas in case\n    of nested models. Other schemas are stored under the \"definitions\"\n    key in the schema of the top-level model.\n\n    \"\"\"\n    simple_schema = {}\n    for name, value in raw_schema[\"properties\"].items():\n        if \"description\" in value:\n            simple_schema[name] = value[\"description\"]\n        elif \"$ref\" in value:\n            refs = value[\"$ref\"].split(\"/\")\n            simple_schema[name] = parse_pydantic_schema(\n                definitions[refs[2]], definitions\n            )\n        else:\n            simple_schema[name] = f\"&lt;{name}&gt;\"\n\n    return simple_schema\n</code></pre>"},{"location":"api/prompts/#outlines.text.prompts.prompt","title":"<code>prompt(fn)</code>","text":"<p>Decorate a function that contains a prompt template.</p> <p>This allows to define prompts in the docstring of a function and simplify their manipulation by providing some degree of encapsulation. It uses the <code>render</code> function internally to render templates.</p> <p>import outlines</p> <p>@outlines.prompt def build_prompt(question): ...    \"I have a ${question}\" ... prompt = build_prompt(\"How are you?\")</p> <p>This API can also be helpful in an \"agent\" context where parts of the prompt are set when the agent is initialized and never modified later. In this situation we can partially apply the prompt function at initialization.</p> <p>import outlines import functools as ft ... @outlines.prompt ... def solve_task(name: str, objective: str, task: str): ...     '''Your name is {{name}}. ..      Your overall objective is to {{objective}}. ...     Please solve the following task: {{task}} ...     ''' ... hal = ft.partial(solve_taks, \"HAL\", \"Travel to Jupiter\")</p>"},{"location":"api/prompts/#outlines.text.prompts.prompt--returns","title":"Returns","text":"<p>A <code>Prompt</code> callable class which will render the template when called.</p> Source code in <code>outlines/text/prompts.py</code> <pre><code>def prompt(fn: Callable) -&gt; Prompt:\n    \"\"\"Decorate a function that contains a prompt template.\n\n    This allows to define prompts in the docstring of a function and simplify their\n    manipulation by providing some degree of encapsulation. It uses the `render`\n    function internally to render templates.\n\n    &gt;&gt;&gt; import outlines\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; @outlines.prompt\n    &gt;&gt;&gt; def build_prompt(question):\n    ...    \"I have a ${question}\"\n    ...\n    &gt;&gt;&gt; prompt = build_prompt(\"How are you?\")\n\n    This API can also be helpful in an \"agent\" context where parts of the prompt\n    are set when the agent is initialized and never modified later. In this situation\n    we can partially apply the prompt function at initialization.\n\n    &gt;&gt;&gt; import outlines\n    &gt;&gt;&gt; import functools as ft\n    ...\n    &gt;&gt;&gt; @outlines.prompt\n    ... def solve_task(name: str, objective: str, task: str):\n    ...     '''Your name is {{name}}.\n    ..      Your overall objective is to {{objective}}.\n    ...     Please solve the following task: {{task}}\n    ...     '''\n    ...\n    &gt;&gt;&gt; hal = ft.partial(solve_taks, \"HAL\", \"Travel to Jupiter\")\n\n    Returns\n    -------\n    A `Prompt` callable class which will render the template when called.\n\n    \"\"\"\n\n    signature = inspect.signature(fn)\n\n    # The docstring contains the template that will be rendered to be used\n    # as a prompt to the language model.\n    docstring = fn.__doc__\n    if docstring is None:\n        raise TypeError(\"Could not find a template in the function's docstring.\")\n\n    template = cast(str, docstring)\n\n    return Prompt(template, signature)\n</code></pre>"},{"location":"api/prompts/#outlines.text.prompts.render","title":"<code>render(template, **values)</code>","text":"<p>Parse a Jinaj2 template and translate it into an Outlines graph.</p> <p>This function removes extra whitespaces and linebreaks from templates to allow users to enter prompts more naturally than if they used Python's constructs directly. See the examples for a detailed explanation.</p>"},{"location":"api/prompts/#outlines.text.prompts.render--examples","title":"Examples","text":"<p>Outlines follow Jinja2's syntax</p> <p>import outlines outline = outlines.render(\"I like {{food}} and {{sport}}\", food=\"tomatoes\", sport=\"tennis\") I like tomatoes and tennis</p> <p>If the first line of the template is empty, <code>render</code> removes it</p> <p>from outlines import render</p> <p>tpl = ''' ... A new string''' tpl ... '\\nA new string' render(tpl) ... 'a new string'</p> <p>Similarly, <code>render</code> ignores linebreaks introduced by placing the closing quotes underneath the text:</p> <p>tpl = ''' ... A new string ... ''' tpl ... '\\nA new string\\n' render(tpl) ... 'A new string'</p> <p>If you want to insert a linebreak at the end of the rendered template, you will need to leave an empty line at the end of the template:</p> <p>tpl = ''' ... A new string ... ... ''' tpl ... '\\nA new string\\n\\n' render(tpl) ... 'A new string\\n'</p> <p><code>render</code> removes the identation in docstrings. This is particularly important when using prompt functions</p> <p>tpl = ''' ...    a string ...    and another string''' tpl ... '\\n   a string\\n   and another string' render(tpl) ... 'a string\\nand another string'</p> <p>The indentation of the first line is assumed to be the same as the second line's</p> <p>tpl = '''a string ...     and another''' tpl ... 'a string\\n    and another' render(tpl) ... 'a string\\nand another'</p> <p>To get a different indentation for the first and the second line, we can start the prompt on the string's second line:</p> <p>tpl = ''' ... First line ...   Second line''' render(tpl) ... 'First Line\\n  Second Line'</p>"},{"location":"api/prompts/#outlines.text.prompts.render--parameters","title":"Parameters","text":"<p>template     A string that contains a template written with the Jinja2 syntax. **values     Map from the variables in the template to their value.</p>"},{"location":"api/prompts/#outlines.text.prompts.render--returns","title":"Returns","text":"<p>A string that contains the rendered template.</p> Source code in <code>outlines/text/prompts.py</code> <pre><code>def render(template: str, **values: Optional[Dict[str, Any]]) -&gt; str:\n    r\"\"\"Parse a Jinaj2 template and translate it into an Outlines graph.\n\n    This function removes extra whitespaces and linebreaks from templates to\n    allow users to enter prompts more naturally than if they used Python's\n    constructs directly. See the examples for a detailed explanation.\n\n    Examples\n    --------\n\n    Outlines follow Jinja2's syntax\n\n    &gt;&gt;&gt; import outlines\n    &gt;&gt;&gt; outline = outlines.render(\"I like {{food}} and {{sport}}\", food=\"tomatoes\", sport=\"tennis\")\n    I like tomatoes and tennis\n\n    If the first line of the template is empty, `render` removes it\n\n    &gt;&gt;&gt; from outlines import render\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; tpl = '''\n    ... A new string'''\n    &gt;&gt;&gt; tpl\n    ... '\\nA new string'\n    &gt;&gt;&gt; render(tpl)\n    ... 'a new string'\n\n    Similarly, `render` ignores linebreaks introduced by placing the closing quotes\n    underneath the text:\n\n    &gt;&gt;&gt; tpl = '''\n    ... A new string\n    ... '''\n    &gt;&gt;&gt; tpl\n    ... '\\nA new string\\n'\n    &gt;&gt;&gt; render(tpl)\n    ... 'A new string'\n\n    If you want to insert a linebreak at the end of the rendered template, you will\n    need to leave an empty line at the end of the template:\n\n    &gt;&gt;&gt; tpl = '''\n    ... A new string\n    ...\n    ... '''\n    &gt;&gt;&gt; tpl\n    ... '\\nA new string\\n\\n'\n    &gt;&gt;&gt; render(tpl)\n    ... 'A new string\\n'\n\n    `render` removes the identation in docstrings. This is particularly important\n    when using prompt functions\n\n    &gt;&gt;&gt; tpl = '''\n    ...    a string\n    ...    and another string'''\n    &gt;&gt;&gt; tpl\n    ... '\\n   a string\\n   and another string'\n    &gt;&gt;&gt; render(tpl)\n    ... 'a string\\nand another string'\n\n    The indentation of the first line is assumed to be the same as the second line's\n\n    &gt;&gt;&gt; tpl = '''a string\n    ...     and another'''\n    &gt;&gt;&gt; tpl\n    ... 'a string\\n    and another'\n    &gt;&gt;&gt; render(tpl)\n    ... 'a string\\nand another'\n\n    To get a different indentation for the first and the second line, we can start the\n    prompt on the string's second line:\n\n    &gt;&gt;&gt; tpl = '''\n    ... First line\n    ...   Second line'''\n    &gt;&gt;&gt; render(tpl)\n    ... 'First Line\\n  Second Line'\n\n    Parameters\n    ----------\n    template\n        A string that contains a template written with the Jinja2 syntax.\n    **values\n        Map from the variables in the template to their value.\n\n    Returns\n    -------\n    A string that contains the rendered template.\n\n    \"\"\"\n    # Dedent, and remove extra linebreak\n    cleaned_template = inspect.cleandoc(template)\n\n    # Add linebreak if there were any extra linebreaks that\n    # `cleandoc` would have removed\n    ends_with_linebreak = template.replace(\" \", \"\").endswith(\"\\n\\n\")\n    if ends_with_linebreak:\n        cleaned_template += \"\\n\"\n\n    # Remove extra whitespaces, except those that immediately follow a newline symbol.\n    # This is necessary to avoid introducing whitespaces after backslash `\\` characters\n    # used to continue to the next line without linebreak.\n    cleaned_template = re.sub(r\"(?![\\r\\n])(\\b\\s+)\", \" \", cleaned_template)\n\n    env = Environment(\n        trim_blocks=True,\n        lstrip_blocks=True,\n        keep_trailing_newline=True,\n        undefined=StrictUndefined,\n    )\n    env.filters[\"name\"] = get_fn_name\n    env.filters[\"description\"] = get_fn_description\n    env.filters[\"source\"] = get_fn_source\n    env.filters[\"signature\"] = get_fn_signature\n    env.filters[\"schema\"] = get_schema\n\n    jinja_template = env.from_string(cleaned_template)\n\n    return jinja_template.render(**values)\n</code></pre>"},{"location":"api/regex/","title":"Regex","text":""},{"location":"api/regex/#outlines.text.generate.regex.Regex","title":"<code>Regex</code>","text":"<p>             Bases: <code>Continuation</code></p> <p>Represents a regex-based generation model.</p> <p><code>Regex</code> instances are constrained generation models that only generate sequences matching a given regex.</p> <p>import outlines.text as text generator = text.generate.regex(model, \"(0|1-9+)\")</p> <p>Sequences can then be generated from a prompt as follows:</p> <p>sequence_1 = generator(\"Return an integer between 0 and 10\") sequence_2 = generator(\"Rate the movie \"Hackers\" on a scale from 0 to 10\")</p> <p>.. note:     Reuse instances of these guided generators (e.g. <code>generator</code> from the     above example) whenever possible, because constructing them has more     overhead than generating token sequences from them.</p> Source code in <code>outlines/text/generate/regex.py</code> <pre><code>class Regex(Continuation):\n    \"\"\"Represents a regex-based generation model.\n\n    `Regex` instances are constrained generation models that only generate\n    sequences matching a given regex.\n\n    &gt;&gt;&gt; import outlines.text as text\n    &gt;&gt;&gt; generator = text.generate.regex(model, \"(0|[1-9][0-9]+)\")\n\n    Sequences can then be generated from a prompt as follows:\n\n    &gt;&gt;&gt; sequence_1 = generator(\"Return an integer between 0 and 10\")\n    &gt;&gt;&gt; sequence_2 = generator(\"Rate the movie \"Hackers\" on a scale from 0 to 10\")\n\n    .. note:\n        Reuse instances of these guided generators (e.g. `generator` from the\n        above example) whenever possible, because constructing them has more\n        overhead than generating token sequences from them.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model,\n        regex_string: str,\n        max_tokens: Optional[int] = None,\n        *,\n        sampler: Optional[\"Sampler\"] = None,\n        stop: Union[str, List[str]] = [],\n        allow_empty_tokens: bool = True,\n        initial_state: Optional[int] = None,\n        final_states: Optional[Set[int]] = None,\n        states_to_token_maps: Optional[Dict[int, Dict[int, int]]] = None,\n        empty_token_ids: Optional[Set[int]] = None,\n        format_fn: Callable[[str], Union[BaseModel, dict, str]] = lambda x: x,\n    ):\n        \"\"\"\n\n        Parameters\n        ----------\n        model\n            The instance of the model used to generate next-token probabilities.\n        regex_string\n            The regex with which the token sampling process is guided/constrained.\n        max_tokens\n            The maximum number of tokens to be sampled.\n        sampler\n            The function used to draw samples.  Defaults to\n            `outlines.text.generate.sample.multinomial`.  See\n            `outlines.text.generate.sample.Sampler` for the expected form of\n            such functions.\n        stop\n            Optional stopping string(s).\n        allow_empty_tokens\n            Allow sampling of tokens corresponding to empty strings.\n        states_to_token_maps\n            Pre-computed map of FSM start states to maps between token ids and their\n            corresponding FSM end states.\n        empty_token_ids\n            Pre-computed set of token ids for tokens that are empty strings.\n        format_fn\n            The function to apply to the generated JSON.\n\n        \"\"\"\n        super().__init__(model, max_tokens, sampler, stop)\n\n        if (\n            states_to_token_maps is None\n            or empty_token_ids is None\n            or initial_state is None\n            or final_states is None\n        ):\n            regex_pattern = interegular.parse_pattern(regex_string)\n            regex_fsm, _ = make_deterministic_fsm(regex_pattern.to_fsm().reduce())\n\n            (\n                self.states_to_token_maps,\n                self.empty_token_ids,\n            ) = create_fsm_index_tokenizer(regex_fsm, model.tokenizer)\n            self.initial_state = regex_fsm.initial\n            self.final_states = regex_fsm.finals\n        else:\n            self.initial_state = initial_state\n            self.final_states = final_states\n            self.states_to_token_maps = states_to_token_maps\n            self.empty_token_ids = empty_token_ids\n\n        # Check whether a terminal path (from the initial state of the FSM to\n        # one of its terminal states) exists, raise an exception otherwise.\n        if not any(\n            self.final_states.intersection(v.values())\n            for v in self.states_to_token_maps.values()\n        ):\n            raise ValueError(\n                \"The vocabulary does not allow us to build a sequence that matches the input regex\"\n            )\n\n        # When an EOS is observed, the last FSM state becomes `-1`.\n        self.last_fsm_states: List[int] = []\n        self.mask_cache: Dict[Tuple[int, int], torch.LongTensor] = {}\n        self.regex_string = regex_string\n        self.allow_empty_tokens = allow_empty_tokens\n        self.format_fn = format_fn\n\n    def create_proposal(\n        self, generated_token_ids: torch.LongTensor, logits: torch.DoubleTensor\n    ) -&gt; torch.DoubleTensor:\n        \"\"\"Modify the next-token logits so that only valid tokens can be generated.\n\n        Parameters\n        ----------\n        generated_token_ids\n            The token ids generated so far.\n        logits\n            The next-token logits.\n\n        \"\"\"\n\n        assert generated_token_ids.ndim == 2\n\n        if len(self.last_fsm_states) == 0:\n            self.last_fsm_states = [self.initial_state for _ in range(logits.shape[0])]\n\n        masks = []\n\n        for i, (token_seq, last_state) in enumerate(\n            zip(\n                generated_token_ids,\n                self.last_fsm_states,\n            )\n        ):\n            if token_seq.shape[0] &gt; 0:\n                # Get the last token that was sampled\n                last_token = int(token_seq[-1])\n\n                if last_token in self.empty_token_ids:\n                    # An empty token was sampled, so the FSM state hasn't changed\n                    next_state = last_state\n                    next_token_ids = list(self.states_to_token_maps[last_state].keys())\n\n                elif last_token != self.model.tokenizer.eos_token_id:\n                    # If we previously ended with an EOS, we shouldn't be\n                    # getting/sampling any more non-EOS tokens.\n                    assert last_state &gt; -1\n\n                    last_token_to_end_state = self.states_to_token_maps[last_state]\n\n                    next_state = last_token_to_end_state[last_token]\n\n                    next_tokens_to_end_states = self.states_to_token_maps.get(\n                        next_state\n                    )\n\n                    if next_tokens_to_end_states is None:\n                        # If there are no transitions from the current state,\n                        # then we must've been in a final state of the FSM.\n                        # We produce EOS tokens from here on.\n                        assert next_state in self.final_states\n                        next_state = -1\n                        next_token_ids = [self.model.tokenizer.eos_token_id]\n                    else:\n                        next_token_ids = list(next_tokens_to_end_states.keys())\n                else:\n                    # Since we already have an EOS, only sample EOS tokes from\n                    # here on.\n                    next_state = -1\n                    next_token_ids = [self.model.tokenizer.eos_token_id]\n            else:\n                # There weren't any previous tokens, so we can't update the state\n                next_state = last_state\n                next_token_ids = list(self.states_to_token_maps[last_state].keys())\n\n            mask = self._get_mask_for_state(\n                next_state, logits.shape[-1], next_token_ids\n            )\n            masks.append(mask)\n            self.last_fsm_states[i] = next_state\n\n        mask = torch.concatenate(masks, dim=0)\n\n        return logits + mask\n\n    def _get_mask_for_state(\n        self, state: int, size: int, next_token_ids: List[int]\n    ) -&gt; torch.LongTensor:\n        mask = self.mask_cache.get((state, size))\n\n        if mask is None:\n            mask = torch.full(\n                (size,),\n                -math.inf,\n                device=self.device,\n            )\n\n            if self.allow_empty_tokens:\n                token_ids = list(self.empty_token_ids) + next_token_ids\n            else:\n                token_ids = next_token_ids\n\n            mask[token_ids] = 0\n            mask = mask.unsqueeze(0)\n            self.mask_cache[(state, size)] = mask\n\n        return mask\n\n    def postprocess_completions(self, completions: List[str]):\n        self.last_fsm_states.clear()\n        results: List[str] = super().postprocess_completions(completions)\n        return [self.format_fn(result) for result in results]\n</code></pre>"},{"location":"api/regex/#outlines.text.generate.regex.Regex.__init__","title":"<code>__init__(model, regex_string, max_tokens=None, *, sampler=None, stop=[], allow_empty_tokens=True, initial_state=None, final_states=None, states_to_token_maps=None, empty_token_ids=None, format_fn=lambda : x)</code>","text":""},{"location":"api/regex/#outlines.text.generate.regex.Regex.__init__--parameters","title":"Parameters","text":"<p>model     The instance of the model used to generate next-token probabilities. regex_string     The regex with which the token sampling process is guided/constrained. max_tokens     The maximum number of tokens to be sampled. sampler     The function used to draw samples.  Defaults to     <code>outlines.text.generate.sample.multinomial</code>.  See     <code>outlines.text.generate.sample.Sampler</code> for the expected form of     such functions. stop     Optional stopping string(s). allow_empty_tokens     Allow sampling of tokens corresponding to empty strings. states_to_token_maps     Pre-computed map of FSM start states to maps between token ids and their     corresponding FSM end states. empty_token_ids     Pre-computed set of token ids for tokens that are empty strings. format_fn     The function to apply to the generated JSON.</p> Source code in <code>outlines/text/generate/regex.py</code> <pre><code>def __init__(\n    self,\n    model,\n    regex_string: str,\n    max_tokens: Optional[int] = None,\n    *,\n    sampler: Optional[\"Sampler\"] = None,\n    stop: Union[str, List[str]] = [],\n    allow_empty_tokens: bool = True,\n    initial_state: Optional[int] = None,\n    final_states: Optional[Set[int]] = None,\n    states_to_token_maps: Optional[Dict[int, Dict[int, int]]] = None,\n    empty_token_ids: Optional[Set[int]] = None,\n    format_fn: Callable[[str], Union[BaseModel, dict, str]] = lambda x: x,\n):\n    \"\"\"\n\n    Parameters\n    ----------\n    model\n        The instance of the model used to generate next-token probabilities.\n    regex_string\n        The regex with which the token sampling process is guided/constrained.\n    max_tokens\n        The maximum number of tokens to be sampled.\n    sampler\n        The function used to draw samples.  Defaults to\n        `outlines.text.generate.sample.multinomial`.  See\n        `outlines.text.generate.sample.Sampler` for the expected form of\n        such functions.\n    stop\n        Optional stopping string(s).\n    allow_empty_tokens\n        Allow sampling of tokens corresponding to empty strings.\n    states_to_token_maps\n        Pre-computed map of FSM start states to maps between token ids and their\n        corresponding FSM end states.\n    empty_token_ids\n        Pre-computed set of token ids for tokens that are empty strings.\n    format_fn\n        The function to apply to the generated JSON.\n\n    \"\"\"\n    super().__init__(model, max_tokens, sampler, stop)\n\n    if (\n        states_to_token_maps is None\n        or empty_token_ids is None\n        or initial_state is None\n        or final_states is None\n    ):\n        regex_pattern = interegular.parse_pattern(regex_string)\n        regex_fsm, _ = make_deterministic_fsm(regex_pattern.to_fsm().reduce())\n\n        (\n            self.states_to_token_maps,\n            self.empty_token_ids,\n        ) = create_fsm_index_tokenizer(regex_fsm, model.tokenizer)\n        self.initial_state = regex_fsm.initial\n        self.final_states = regex_fsm.finals\n    else:\n        self.initial_state = initial_state\n        self.final_states = final_states\n        self.states_to_token_maps = states_to_token_maps\n        self.empty_token_ids = empty_token_ids\n\n    # Check whether a terminal path (from the initial state of the FSM to\n    # one of its terminal states) exists, raise an exception otherwise.\n    if not any(\n        self.final_states.intersection(v.values())\n        for v in self.states_to_token_maps.values()\n    ):\n        raise ValueError(\n            \"The vocabulary does not allow us to build a sequence that matches the input regex\"\n        )\n\n    # When an EOS is observed, the last FSM state becomes `-1`.\n    self.last_fsm_states: List[int] = []\n    self.mask_cache: Dict[Tuple[int, int], torch.LongTensor] = {}\n    self.regex_string = regex_string\n    self.allow_empty_tokens = allow_empty_tokens\n    self.format_fn = format_fn\n</code></pre>"},{"location":"api/regex/#outlines.text.generate.regex.Regex.create_proposal","title":"<code>create_proposal(generated_token_ids, logits)</code>","text":"<p>Modify the next-token logits so that only valid tokens can be generated.</p>"},{"location":"api/regex/#outlines.text.generate.regex.Regex.create_proposal--parameters","title":"Parameters","text":"<p>generated_token_ids     The token ids generated so far. logits     The next-token logits.</p> Source code in <code>outlines/text/generate/regex.py</code> <pre><code>def create_proposal(\n    self, generated_token_ids: torch.LongTensor, logits: torch.DoubleTensor\n) -&gt; torch.DoubleTensor:\n    \"\"\"Modify the next-token logits so that only valid tokens can be generated.\n\n    Parameters\n    ----------\n    generated_token_ids\n        The token ids generated so far.\n    logits\n        The next-token logits.\n\n    \"\"\"\n\n    assert generated_token_ids.ndim == 2\n\n    if len(self.last_fsm_states) == 0:\n        self.last_fsm_states = [self.initial_state for _ in range(logits.shape[0])]\n\n    masks = []\n\n    for i, (token_seq, last_state) in enumerate(\n        zip(\n            generated_token_ids,\n            self.last_fsm_states,\n        )\n    ):\n        if token_seq.shape[0] &gt; 0:\n            # Get the last token that was sampled\n            last_token = int(token_seq[-1])\n\n            if last_token in self.empty_token_ids:\n                # An empty token was sampled, so the FSM state hasn't changed\n                next_state = last_state\n                next_token_ids = list(self.states_to_token_maps[last_state].keys())\n\n            elif last_token != self.model.tokenizer.eos_token_id:\n                # If we previously ended with an EOS, we shouldn't be\n                # getting/sampling any more non-EOS tokens.\n                assert last_state &gt; -1\n\n                last_token_to_end_state = self.states_to_token_maps[last_state]\n\n                next_state = last_token_to_end_state[last_token]\n\n                next_tokens_to_end_states = self.states_to_token_maps.get(\n                    next_state\n                )\n\n                if next_tokens_to_end_states is None:\n                    # If there are no transitions from the current state,\n                    # then we must've been in a final state of the FSM.\n                    # We produce EOS tokens from here on.\n                    assert next_state in self.final_states\n                    next_state = -1\n                    next_token_ids = [self.model.tokenizer.eos_token_id]\n                else:\n                    next_token_ids = list(next_tokens_to_end_states.keys())\n            else:\n                # Since we already have an EOS, only sample EOS tokes from\n                # here on.\n                next_state = -1\n                next_token_ids = [self.model.tokenizer.eos_token_id]\n        else:\n            # There weren't any previous tokens, so we can't update the state\n            next_state = last_state\n            next_token_ids = list(self.states_to_token_maps[last_state].keys())\n\n        mask = self._get_mask_for_state(\n            next_state, logits.shape[-1], next_token_ids\n        )\n        masks.append(mask)\n        self.last_fsm_states[i] = next_state\n\n    mask = torch.concatenate(masks, dim=0)\n\n    return logits + mask\n</code></pre>"},{"location":"api/regex/#outlines.text.generate.regex.choice","title":"<code>choice(model, choices, max_tokens=None, *, sampler=None, allow_empty_tokens=True)</code>","text":"<p>Choose between different sequences.</p> <p>.. note:     Reuse instances of these guided generators whenever possible,     because constructing them has more overhead than generating     token sequences from them.  See the docstring for <code>Regex</code>.</p>"},{"location":"api/regex/#outlines.text.generate.regex.choice--parameters","title":"Parameters","text":"<p>model     The language model to use to compute the next-token logits. max_tokens     The maximum number of tokens to generate. sampler     The function used to draw samples.  Defaults to     <code>outlines.text.generate.sample.multinomial</code>.  See     <code>outlines.text.generate.sample.Sampler</code> for the expected form of     such functions. allow_empty_tokens     Allow sampling of tokens corresponding to empty strings.</p> Source code in <code>outlines/text/generate/regex.py</code> <pre><code>def choice(\n    model,\n    choices: List[str],\n    max_tokens: Optional[int] = None,\n    *,\n    sampler: Optional[\"Sampler\"] = None,\n    allow_empty_tokens: bool = True,\n):\n    \"\"\"Choose between different sequences.\n\n    .. note:\n        Reuse instances of these guided generators whenever possible,\n        because constructing them has more overhead than generating\n        token sequences from them.  See the docstring for `Regex`.\n\n    Parameters\n    ----------\n    model\n        The language model to use to compute the next-token logits.\n    max_tokens\n        The maximum number of tokens to generate.\n    sampler\n        The function used to draw samples.  Defaults to\n        `outlines.text.generate.sample.multinomial`.  See\n        `outlines.text.generate.sample.Sampler` for the expected form of\n        such functions.\n    allow_empty_tokens\n        Allow sampling of tokens corresponding to empty strings.\n    \"\"\"\n    regex_str = r\"(\" + r\"|\".join(choices) + r\")\"\n    return Regex(\n        model,\n        regex_str,\n        max_tokens,\n        sampler=sampler,\n        allow_empty_tokens=allow_empty_tokens,\n    )\n</code></pre>"},{"location":"api/regex/#outlines.text.generate.regex.format","title":"<code>format(model, python_type, max_tokens=None, *, sampler=None, allow_empty_tokens=True)</code>","text":"<p>Generate integers.</p> <p>The regex used to constrain the generation optionally matches plus or minus signs and forbids leading zeros (even if the <code>int</code> function in Python allows them).</p> <p>.. note:     Reuse instances of these guided generators whenever possible,     because constructing them has more overhead than generating     token sequences from them.  See the docstring for <code>Regex</code>.</p>"},{"location":"api/regex/#outlines.text.generate.regex.format--parameters","title":"Parameters","text":"<p>model     The language model to use to compute the next-token logits. python_type     The format in which the output is expected, defined as a Python type. max_tokens     The maximum number of tokens to generate. sampler     The function used to draw samples.  Defaults to     <code>outlines.text.generate.sample.multinomial</code>.  See     <code>outlines.text.generate.sample.Sampler</code> for the expected form of     such functions. allow_empty_tokens     Allow sampling of tokens corresponding to empty strings.</p> Source code in <code>outlines/text/generate/regex.py</code> <pre><code>def format(\n    model,\n    python_type,\n    max_tokens: Optional[int] = None,\n    *,\n    sampler: Optional[\"Sampler\"] = None,\n    allow_empty_tokens: bool = True,\n):\n    \"\"\"Generate integers.\n\n    The regex used to constrain the generation optionally matches plus or minus\n    signs and forbids leading zeros (even if the `int` function in Python allows\n    them).\n\n    .. note:\n        Reuse instances of these guided generators whenever possible,\n        because constructing them has more overhead than generating\n        token sequences from them.  See the docstring for `Regex`.\n\n    Parameters\n    ----------\n    model\n        The language model to use to compute the next-token logits.\n    python_type\n        The format in which the output is expected, defined as a Python type.\n    max_tokens\n        The maximum number of tokens to generate.\n    sampler\n        The function used to draw samples.  Defaults to\n        `outlines.text.generate.sample.multinomial`.  See\n        `outlines.text.generate.sample.Sampler` for the expected form of\n        such functions.\n    allow_empty_tokens\n        Allow sampling of tokens corresponding to empty strings.\n\n    \"\"\"\n    regex_str = python_types_to_regex(python_type)\n    return Regex(\n        model,\n        regex_str,\n        max_tokens,\n        sampler=sampler,\n        allow_empty_tokens=allow_empty_tokens,\n    )\n</code></pre>"},{"location":"api/regex/#outlines.text.generate.regex.json","title":"<code>json(model, schema_object, max_tokens=None, *, sampler=None, allow_empty_tokens=True)</code>","text":"<p>Generate a text sequence that follows a JSON schema or Pydantic model.</p> <p>.. note:     Reuse instances of these guided generators whenever possible,     because constructing them has more overhead than generating     token sequences from them. See the docstring for <code>Regex</code>.</p>"},{"location":"api/regex/#outlines.text.generate.regex.json--parameters","title":"Parameters","text":"<p>model     The language model to use to compute the next-token logits. schema     The JSON schema, Pydantic model or function (signature) that guides the     generation. max_tokens     The maximum number of tokens to generate. sampler     The function used to draw samples.  Defaults to     <code>outlines.text.generate.sample.multinomial</code>.  See     <code>outlines.text.generate.sample.Sampler</code> for the expected form of     such functions. allow_empty_tokens     Allow sampling of tokens corresponding to empty strings.</p> Source code in <code>outlines/text/generate/regex.py</code> <pre><code>def json(\n    model,\n    schema_object: Union[str, BaseModel, Callable],\n    max_tokens: Optional[int] = None,\n    *,\n    sampler: Optional[\"Sampler\"] = None,\n    allow_empty_tokens: bool = True,\n) -&gt; Union[dict, BaseModel]:\n    \"\"\"Generate a text sequence that follows a JSON schema or Pydantic model.\n\n    .. note:\n        Reuse instances of these guided generators whenever possible,\n        because constructing them has more overhead than generating\n        token sequences from them. See the docstring for `Regex`.\n\n    Parameters\n    ---------\n    model\n        The language model to use to compute the next-token logits.\n    schema\n        The JSON schema, Pydantic model or function (signature) that guides the\n        generation.\n    max_tokens\n        The maximum number of tokens to generate.\n    sampler\n        The function used to draw samples.  Defaults to\n        `outlines.text.generate.sample.multinomial`.  See\n        `outlines.text.generate.sample.Sampler` for the expected form of\n        such functions.\n    allow_empty_tokens\n        Allow sampling of tokens corresponding to empty strings.\n\n    \"\"\"\n    if isinstance(schema_object, type(BaseModel)):\n        schema = pyjson.dumps(schema_object.model_json_schema())\n        format_fn = lambda x: schema_object.model_validate(pyjson.loads(x))\n    elif callable(schema_object):\n        schema = pyjson.dumps(get_schema_from_signature(schema_object))\n        # TODO: Convert string fields to their respective types\n        format_fn = lambda x: pyjson.loads(x)\n    else:\n        format_fn = lambda x: x\n\n    regex_str = build_regex_from_object(schema)\n\n    return Regex(\n        model,\n        regex_str,\n        max_tokens,\n        sampler=sampler,\n        allow_empty_tokens=allow_empty_tokens,\n        format_fn=format_fn,\n    )\n</code></pre>"},{"location":"api/regex/#outlines.text.generate.regex.regex","title":"<code>regex(model, regex_string, max_tokens=None, *, sampler=None, allow_empty_tokens=True)</code>","text":"<p>Generate text sequences that match the input regex.</p> <p>.. note:     Reuse instances of these guided generators whenever possible,     because constructing them has more overhead than generating     token sequences from them.  See the docstring for <code>Regex</code>.</p>"},{"location":"api/regex/#outlines.text.generate.regex.regex--parameters","title":"Parameters","text":"<p>model     The language model to use to compute the next-token logits. regex_string     The regular expression that generated expressions must match. max_tokens     The maximum number of tokens to generate. sampler     The function used to draw samples.  Defaults to     <code>outlines.text.generate.sample.multinomial</code>.  See     <code>outlines.text.generate.sample.Sampler</code> for the expected form of     such functions. allow_empty_tokens     Allow sampling of tokens corresponding to empty strings.</p> Source code in <code>outlines/text/generate/regex.py</code> <pre><code>def regex(\n    model,\n    regex_string: str,\n    max_tokens: Optional[int] = None,\n    *,\n    sampler: Optional[\"Sampler\"] = None,\n    allow_empty_tokens: bool = True,\n):\n    \"\"\"Generate text sequences that match the input regex.\n\n    .. note:\n        Reuse instances of these guided generators whenever possible,\n        because constructing them has more overhead than generating\n        token sequences from them.  See the docstring for `Regex`.\n\n    Parameters\n    ----------\n    model\n        The language model to use to compute the next-token logits.\n    regex_string\n        The regular expression that generated expressions must match.\n    max_tokens\n        The maximum number of tokens to generate.\n    sampler\n        The function used to draw samples.  Defaults to\n        `outlines.text.generate.sample.multinomial`.  See\n        `outlines.text.generate.sample.Sampler` for the expected form of\n        such functions.\n    allow_empty_tokens\n        Allow sampling of tokens corresponding to empty strings.\n\n    \"\"\"\n    return Regex(\n        model,\n        regex_string,\n        max_tokens,\n        sampler=sampler,\n        allow_empty_tokens=allow_empty_tokens,\n    )\n</code></pre>"},{"location":"api/sample/","title":"Sample","text":""},{"location":"api/sample/#outlines.text.generate.sample.greedy","title":"<code>greedy(logits, samples, *_)</code>","text":"<p>Greedy Sampling algorithm.</p> <p>Greedy sampling consists in choosing the token with the largest likelihood at every step.</p>"},{"location":"api/sample/#outlines.text.generate.sample.greedy--parameters","title":"Parameters","text":"<p>logits     A tensor of shape <code>(n_seqs, vocab_size,)</code> that represents the     probability distribution of the next token over the vocabulary. samples     The number of sequences to produce.  In this case, the top-<code>samples</code>     logit values are returned. rng     A random number generator.</p>"},{"location":"api/sample/#outlines.text.generate.sample.greedy--returns","title":"Returns","text":"<p>The ids of the sampled tokens having shape <code>(samples, n_seqs)</code>.</p> Source code in <code>outlines/text/generate/sample.py</code> <pre><code>def greedy(logits: torch.DoubleTensor, samples: int, *_) -&gt; torch.DoubleTensor:\n    \"\"\"Greedy Sampling algorithm.\n\n    Greedy sampling consists in choosing the token with the largest\n    likelihood at every step.\n\n    Parameters\n    ----------\n    logits\n        A tensor of shape ``(n_seqs, vocab_size,)`` that represents the\n        probability distribution of the next token over the vocabulary.\n    samples\n        The number of sequences to produce.  In this case, the top-`samples`\n        logit values are returned.\n    rng\n        A random number generator.\n\n    Returns\n    -------\n    The ids of the sampled tokens having shape ``(samples, n_seqs)``.\n\n    \"\"\"\n    if samples == 1:\n        next_token_ids = torch.argmax(logits, dim=-1, keepdim=True).T\n    else:\n        next_token_ids = torch.topk(\n            logits, samples, dim=-1, largest=True, sorted=True\n        ).indices.T\n\n    return next_token_ids\n</code></pre>"},{"location":"api/sample/#outlines.text.generate.sample.multinomial","title":"<code>multinomial(logits, samples, rng)</code>","text":"<p>Multinomial sampling algorithm.</p> <p>Multinomial sampling consists in randomly sampling the next token assuming its distribution is a Categorical distribution parametrized by the next-token logits.</p>"},{"location":"api/sample/#outlines.text.generate.sample.multinomial--parameters","title":"Parameters","text":"<p>logits     A tensor of shape <code>(n_seqs, vocab_size,)</code> that represents the     probability distribution of the next token over the vocabulary. samples     The number of sequences to sample. rng     A random number generator.</p>"},{"location":"api/sample/#outlines.text.generate.sample.multinomial--returns","title":"Returns","text":"<p>The ids of the sampled tokens having shape <code>(samples, n_seqs)</code>.</p> Source code in <code>outlines/text/generate/sample.py</code> <pre><code>def multinomial(\n    logits: torch.DoubleTensor, samples: int, rng: torch.Generator\n) -&gt; torch.DoubleTensor:\n    \"\"\"Multinomial sampling algorithm.\n\n    Multinomial sampling consists in randomly sampling the next token assuming\n    its distribution is a Categorical distribution parametrized by the\n    next-token logits.\n\n    Parameters\n    ----------\n    logits\n        A tensor of shape ``(n_seqs, vocab_size,)`` that represents the\n        probability distribution of the next token over the vocabulary.\n    samples\n        The number of sequences to sample.\n    rng\n        A random number generator.\n\n    Returns\n    -------\n    The ids of the sampled tokens having shape ``(samples, n_seqs)``.\n\n    \"\"\"\n    probs = torch.nn.functional.softmax(logits, dim=-1)\n    # next_token_ids = torch.multinomial(probs, num_samples=samples, generator=rng)\n    next_token_ids = vectorized_random_choice(rng, probs, samples)\n    return next_token_ids\n</code></pre>"},{"location":"api/sample/#outlines.text.generate.sample.vectorized_random_choice","title":"<code>vectorized_random_choice(rng, p, samples=1)</code>","text":"<p>Vectorized implementation of <code>np.random.choice</code>.</p> <p><code>np.random.choice</code> does not support arrays of probability. This implements the equivalent of this function where the <code>p</code> argument can be a matrix.</p>"},{"location":"api/sample/#outlines.text.generate.sample.vectorized_random_choice--note","title":"Note","text":"<p><code>torch.searchsorted</code> may be more efficient, but it is not implemented for every backend, for instance MPS.</p>"},{"location":"api/sample/#outlines.text.generate.sample.vectorized_random_choice--parameters","title":"Parameters","text":"<p>rng     Torch random number Generator instance p     An array of probability of shape <code>(num_probability_vectors, num_items)</code>     that must sum to 1. samples     The number of samples to take for each probability vector.</p>"},{"location":"api/sample/#outlines.text.generate.sample.vectorized_random_choice--returns","title":"Returns","text":"<p>An array of shape <code>(num_samples, batch_size)</code></p> Source code in <code>outlines/text/generate/sample.py</code> <pre><code>def vectorized_random_choice(\n    rng: torch.Generator,\n    p: torch.FloatTensor,\n    samples: int = 1,\n):\n    \"\"\"Vectorized implementation of `np.random.choice`.\n\n    `np.random.choice` does not support arrays of probability. This implements\n    the equivalent of this function where the `p` argument can be a matrix.\n\n    Note\n    ----\n    `torch.searchsorted` may be more efficient, but it is not implemented for\n    every backend, for instance MPS.\n\n    Parameters\n    ----------\n    rng\n        Torch random number Generator instance\n    p\n        An array of probability of shape ``(num_probability_vectors, num_items)``\n        that must sum to 1.\n    samples\n        The number of samples to take for each probability vector.\n\n    Returns\n    -------\n    An array of shape ``(num_samples, batch_size)``\n\n    \"\"\"\n    cumsum = torch.unsqueeze(p.cumsum(axis=-1), 0)\n    rand = torch.rand(\n        (samples,) + p.shape[:-1] + (1,), generator=rng, device=rng.device\n    )\n    idx = (cumsum &lt; rand).sum(axis=-1)\n\n    return idx\n</code></pre>"},{"location":"cookbook/","title":"Cookbook","text":""},{"location":"examples/","title":"Examples","text":"<ul> <li>Dating Profile: Build dating profiles from descriptions using prompt templating and JSON-guided generation.</li> </ul>"},{"location":"examples/dating_profiles/","title":"Generate a dating profile from a description","text":"<p>In this example we will see how we can use Outlines to generate synthetic data for a dating application. This example was originally contributed by Vibhor Kumar.</p> <pre><code>from dataclasses import dataclass\nfrom enum import Enum\n\nimport torch\nimport transformers\nfrom pydantic import BaseModel, conlist, constr\n\nimport outlines.models as models\nimport outlines.text as text\n</code></pre>"},{"location":"examples/dating_profiles/#defining-the-profile-with-pydantic","title":"Defining the profile with Pydantic","text":"<p>Here a dating profile will consist in a biography, a job, a list of interests and two question-answer pairs. The questions are written in advance by the team, and the users are asked to provide an answer:</p> <pre><code>class QuestionChoice(str, Enum):\n    A = \"The key to my heart is\"\n    B = \"The first item on my bucket list is\"\n    C = \"Perks of dating me\"\n    D = \"Message me if you also love\"\n    E = \"People would describe me as\"\n    F = \"I can beat you in a game of\"\n\n@dataclass\nclass QuestionAnswer:\n    question: QuestionChoice\n    answer: str\n</code></pre> <p>Users need to provide a short biography, with a minimum of 10 and a maximum of 300 characters. The application also limits job descriptions to 50 characters. In addition to the question-answer pairs, the user is required to provide a list of between 1 and 5 interests:</p> <pre><code>class DatingProfile(BaseModel):\n    bio: constr(str, min_length=10, max_length=300)\n    job: constr(str, max_lengt=50)\n    interests: conlist(str, min_length=1, max_length=5)  # type: ignore\n    qna1: QuestionAnswer\n    qna2: QuestionAnswer\n</code></pre>"},{"location":"examples/dating_profiles/#prompt-template-and-examples","title":"Prompt template and examples","text":"<p>We will ask the model to generate profiles from a high-level description:</p> <pre><code>@dataclass\nclass Example:\n    description: str\n    profile: DatingProfile\n</code></pre> <p>We will use Outlines' prompt templating abilities to generate the prompt for us. This help clearly separate the general prompting logic from what is specific to an example.</p> <pre><code>@text.prompt\ndef dating_profile_prompt(description: str, examples: list[Example]):\n    \"\"\"\n    You are a world-renowned matchmaker who understands the modern dating\n    market. Your job is to generate dating app profiles for male clients\n    interested in women based on a provided description. The profiles should be\n    authentic, show off their strengths, and maximize their likelihood of\n    getting matches on dating apps.  Here are some examples of past clients that\n    you have successfully created profiles for:\n\n    {% for example in examples %}\n    Description:\n    {{ example.description }}\n    Profile:\n    {{ example.profile }}\n    {% endfor %}\n\n    Here is the new client who you need to create a profile for:\n    Description: {{ description }}\n    Profile:\n    \"\"\"\n</code></pre> <p>We will provide the model with several few-shot examples:</p> <pre><code>samples: list[Example] = [\n    Example(\n        description=\"I'm an author and former professional soccer player living in Seattle who publishes popular fiction books. A typical day for me starts by hanging out with my cat, drinking a coffee, and reading as much as I can in a few hours. Then, I'll prepare a quick smoothie before starting to write for a few hours, take a break with soccer or running a few miles, and finally meet friends for dinner at a new, hip restaurant in the evening. Sometimes we go axe-throwing afterwards, or play poker, or watch a comedy show, or visit a dive bar. On my vacations, I travel extensively to countries South America, Europe, and Asia, with the goal of visiting them all!\",\n        profile=DatingProfile(\n            bio=\"Adventurer, dreamer, author, and soccer enthusiast. Life\u2019s too short to waste time so I make the most of each day by exploring new places and playing with my friends on the pitch. What\u2019s your favorite way to get out and have fun?\",\n            job=\"Famous Soccer Player -&gt; Famous Author\",\n            interests=[\"Soccer\", \"Travel\", \"Friends\", \"Books\", \"Fluffy Animals\"],\n            qna1=QuestionAnswer(\n                question=QuestionChoice.B, answer=\"swim in all seven oceans!\"\n            ),\n            qna2=QuestionAnswer(\n                question=QuestionChoice.E,\n                answer=\"fun-loving, adventurous, and a little bit crazy\",\n            ),\n        ),\n    ),\n    Example(\n        description=\"I run my company and build houses for a living. I'm a big fan of the outdoors and love to go hiking, camping, and fishing. I don't like video games, but do like to watch movies. My love language is home-cooked food, and I'm looking for someone who isn't afraid to get their hands dirty.\",\n        profile=DatingProfile(\n            bio=\"If you're looking for a Montana man who loves to get outdoors and hunt, and who's in-tune with his masculinity then I'm your guy!\",\n            job=\"House Construction Manager / Entrepreneur\",\n            interests=[\"Hunting\", \"Hiking\", \"The outdoors\", \"Home-cooked food\"],\n            qna1=QuestionAnswer(question=QuestionChoice.A, answer=\"food made at home\"),\n            qna2=QuestionAnswer(\n                question=QuestionChoice.C,\n                answer=\"having a man in your life who can fix anything\",\n            ),\n        ),\n    ),\n    Example(\n        description=\"I run my own Youtube channel with 10M subscribers. I love working with kids, and my audience skews pretty young too. In my free time, I play Fortnite and Roblox. I'm looking for someone who is also a gamer and likes to have fun. I'm learning Japanese in my free time as well as how to cook.\",\n        profile=DatingProfile(\n            bio=\"Easy on the eyes (find me on Youtube!) and great with kids. What more do you need?\",\n            job=\"Youtuber 10M+ subscribers\",\n            interests=[\"Kids\", \"Gaming\", \"Japanese\"],\n            qna1=QuestionAnswer(question=QuestionChoice.D, answer=\"anime and gaming!\"),\n            qna2=QuestionAnswer(question=QuestionChoice.F, answer=\"Fortnite, gg ez\"),\n        ),\n    ),\n]\n</code></pre>"},{"location":"examples/dating_profiles/#load-the-model","title":"Load the model","text":"<p>We will use Mosaic's MPT-7B model (requires 13GB of GPU memory) which can fit on a single GPU with a reasonable context window. We initialize it with Outlines:</p> <pre><code>config = transformers.AutoConfig.from_pretrained(\n    \"mosaicml/mpt-7b-8k-instruct\", trust_remote_code=True\n)\nconfig.init_device = \"meta\"\nmodel = models.transformers(\n    model_name=\"mosaicml/mpt-7b-8k-instruct\",\n    device=\"cuda\",\n    model_kwargs={\n        \"config\": config,\n        \"trust_remote_code\": True,\n        \"torch_dtype\": torch.bfloat16,\n        \"device_map\": {\"\": 0},\n    },\n)\n</code></pre>"},{"location":"examples/dating_profiles/#json-guided-generation-of-profiles","title":"JSON-guided generation of profiles","text":"<p>We will now generate a dating profile from a textual description of oneself:</p> <pre><code>new_description = \"\"\"I'm a laid-back lawyer who spends a lot of his free-time\ngaming. I work in a corporate office, but ended up here after the start-up  I\ncofounded got acquired, so still play ping pong with my cool coworkers every\nday.  I have a bar at home where I make cocktails, which is great for\nentertaining  friends. I secretly like to wear suits and get a new one tailored\nevery few  months. I also like weddings because I get to wear those suits, and\nit's  a good excuse for a date. I watch the latest series because I'm paying,\nwith my hard-earned money, for every streaming service.\"\"\"\n\nprompt = dating_profile_prompt(new_description, samples)\nprofile = text.generate.json(model, DatingProfile)(prompt)\nparsed_profile = DatingProfile.model_validate_json(profile)\n</code></pre>"},{"location":"examples/dating_profiles/#results","title":"Results","text":"<p>Here are a couple of results:</p> <pre><code>{\n    \"bio\": \"\"\"I'm an ambitious lawyer with a casual and fashionable style. I love\n    games and sports, but my true passion is preparing refreshing cocktails at\n    home and dressing to the nines at weddings. I'm currently looking for a woman\n    to show a good time to and get a kiss on the opulent suit I just had made.\n    Send resume to this inbox.\"\"\",\n    \"job\": \"Lawyer\",\n    \"interests\":\n    [\n        \"Stylish guys\",\n        \"Gaming\",\n        \"Ping pong\",\n        \"Cocktails\",\n        \"Weddings\"\n    ],\n    \"qna1\":\n    {\n        \"question\": \"The first item on my bucket list is\",\n        \"answer\": \"be married and have a family.\"\n    },\n    \"qna2\":\n    {\n        \"question\": \"People would describe me as\",\n        \"answer\": \"charming, stylish, and funny.\"\n    }\n}\n</code></pre> <pre><code>{\n    \"bio\": \"\"\"I\u2019m a sexy lawyer with time on my hands. I love to game and\n    play ping pong, but the real reason you should swipe to the right\n    is because I look great in a suit. Who doesn\u2019t love a man in a\n    suit? Just saying. Send me a message if you think it\u2019s time to take\n    your dating life to the next level.\"\"\",\n    \"job\": \"Lawyer\",\n    \"interests\":\n    [\n        \"Gaming\",\n        \"Ping Pong\",\n        \"Tailored Suits\",\n        \"Weddings\",\n        \"Streaming Services\"\n    ],\n    \"qna1\":\n    {\n        \"question\": \"The first item on my bucket list is\",\n        \"answer\": \"simulate space but stay alive for as long as possible\"\n    },\n    \"qna2\":\n    {\n        \"question\": \"People would describe me as\",\n        \"answer\": \"easy-going, a little nerdy but with a mature essence\"\n    }\n}\n</code></pre>"},{"location":"reference/","title":"Reference","text":""},{"location":"reference/#constrained-generation","title":"Constrained generation","text":"<p>While LLM capabilities are increasingly impressive, we can make their output more reliable by steering the generation. Outlines thus offers mechanisms to specify high level constraints on text completions by generative language models.</p> <p>Stopping sequence By default, language models stop generating tokens after and  token was generated, or after a set maximum number of tokens. Their output can be verbose, and for practical purposes it is often necessary to stop the generation after a given sequence has been found instead. You can use the stop_at keyword argument when calling the model with a prompt: <pre><code>import outlines.models as models\n\ncomplete = models.text_completion.openai(\"text-davinci-002\")\nexpert = complete(\"Name an expert in quantum gravity.\", stop_at=[\"\\n\", \".\"])\n</code></pre>"},{"location":"reference/choices/","title":"Multiple choices","text":"<p>Choice between different options In some cases we know the output is to be chosen between different options. We can restrict the completion\u2019s output to these choices using the is_in keyword argument:</p> <pre><code>import outlines.models as models\n\ncomplete = models.text_completion.openai(\"text-davinci-002\")\nanswer = complete(\n    \"Pick the odd word out: skirt, dress, pen, jacket\",\n    is_in=[\"skirt\", \"dress\", \"pen\", \"jacket\"]\n)\n</code></pre>"},{"location":"reference/json/","title":"Make the LLM follow a JSON Schema","text":"<p>Outlines can make any open source model return a JSON object that follows a structure that is specified by the user. This is useful whenever we want the output of the model to be processed by code downstream: code does not understand natural language but rather the structured language it has been programmed to understand.</p> <p>There are mostly two reasons why someone would want to get an output formatted as JSON from a LLM:</p> <ol> <li>Parse the answer (e.g. with Pydantic), store it somewhere, return it to a user, etc.</li> <li>Call a function with the result</li> </ol> <p>Outlines has you covered in both cases! Indeed, to define the structure of the JSON you want the model to follow you can either provide a Pydantic model, or a function. No need to duplicate code!</p>"},{"location":"reference/json/#using-pydantic","title":"Using Pydantic","text":"<p>Outlines can infer the structure of the output from a Pydantic model. The result is an instance of the model that contains the values returned by the LLM:</p> <pre><code>from pydantic import BaseModel\n\nfrom outlines import models\nfrom outlines import text\n\n\nclass User(BaseModel):\n    name: str\n    last_name: str\n    id: int\n\n\nmodel = models.transformers(\"mistralai/Mistral-7B\")\ngenerator = text.generate.json(model, User)\nresult = generator(\"Create a user profile with the fields name, last_name and id\")\nprint(result)\n# User(name=\"John\", last_name=\"Doe\", id=11)\n</code></pre>"},{"location":"reference/json/#from-a-functions-signature","title":"From a function's signature","text":"<p>Outlines can infer the structure of the output from the signature of a function. The result is a dictionary, and can be passed directly to the function using the usual dictionary expansion syntax <code>**</code>:</p> <pre><code>from outlines import models\nfrom outlines import text\n\ndef add(a: int, b: int):\n    return a + b\n\nmodel = models.transformers(\"mistralai/Mistral-7B\")\ngenerator = text.generate.json(model, add)\nresult = generator(\"Return two integers named a and b respectively. a is odd and b even.\")\n\nprint(add(**result))\n# 3\n</code></pre> <p>A great advantage of passing functions directly to specify the structure is that the structure of the LLM will change with the function's definition. No need to change the code at several places!</p>"},{"location":"reference/openai_text_generation/","title":"Generate text with the OpenAI API","text":"<p>Outlines is focused on \ud83d\udd13 models, but includes an OpenAI integration nevertheless. You can instantiate a model very simply by calling the outlines.models.openai function, with either a chat or non chat model:</p> <pre><code>from outlines import models\n\nmodel = models.openai(\"text-davinci-003\")\nmodel = models.openai(\"gpt4\")\n\nprint(type(model))\n# OpenAIAPI\n</code></pre> <p>Note</p> <p>It is currently not possible to pass a system message to the model. If that is something you need, please open an Issue or, better, submit a Pull Request.</p> <p>The OpenAI integration supports the following features:</p> <ul> <li>The ability to stop the generation when a specified sequence is found \ud83d\udd17</li> <li>The ability to choose between different choices \ud83d\udd17</li> <li>Vectorization, i.e. the ability to pass an array of prompts and execute all requests concurrently \ud83d\udd17</li> </ul>"},{"location":"reference/openai_text_generation/#stop-when-a-sequence-is-found","title":"Stop when a sequence is found","text":"<p>The OpenAI API tends to be chatty and it can be useful to stop the generation once a given sequence has been found, instead of paying for the extra tokens and needing to post-process the output. For instance if you only to generate a single sentence:</p> <pre><code>from outlines import models\n\nmodel = models.openai(\"text-davinci-003\")\nresponse = model(\"Write a sentence\", stop_at=['.'])\n</code></pre>"},{"location":"reference/openai_text_generation/#multiple-choices","title":"Multiple choices","text":"<p>It can be difficult to deal with a classification problem with the OpenAI API. However well you prompt the model, chances are you are going to have to post-process the output anyway. Sometimes the model will even make up choices. Outlines allows you to guarantee that the output of the model will be within a set of choices you specify:</p> <pre><code>from outlines import models\n\nprompt = \"\"\"\nReview: The OpenAI API is very limited. It does not allow me to do guided generation properly.\nQuestion: What is the overall sentiment of this review?\nAnswer:\n\"\"\"\n\nmodel = models.openai(\"text-davinci-003\")\nresponse = model(prompt, is_in=['Positive', 'Negative'])\n</code></pre>"},{"location":"reference/openai_text_generation/#vectorized-calls","title":"Vectorized calls","text":"<p>A unique feature of Outlines is that calls to the OpenAI API are vectorized (In the NumPy sense of the word). In plain English this means that you can call an Openai model with an array of prompts with arbitrary shape to an OpenAI model and it will return an array of answers. All calls are executed concurrently, which means this takes roughly the same time as calling the model with a single prompt:</p> <pre><code>from outlines import models\nfrom outlines import text\n\n@text.prompt\ndef template(input_numbers):\n    \"\"\"Use these numbers and basic arithmetic to get 24 as a result:\n\n    Input: {{ input_numbers }}\n    Steps: \"\"\"\n\nprompts = [\n    template([1, 2, 3]),\n    template([5, 9, 7]),\n    template([10, 12])\n]\n\nmodel = models.openai(\"text-davinci-003\")\nresults = model(prompts)\nprint(results.shape)\n# (3,)\n\nprint(type(results))\n# &lt;class 'numpy.ndarray'&gt;\n\nprint(results)\n# [\n#     \"\\n1. 1 + 2 x 3 = 7\\n2. 7 + 3 x 4 = 19\\n3. 19 + 5 = 24\",\n#     \"\\n1. Add the three numbers together: 5 + 9 + 7 = 21\\n2. Subtract 21 from 24: 24 - 21 = 3\\n3. Multiply the remaining number by itself: 3 x 3 = 9\\n4. Add the number with the multiplication result: 21 + 9 = 24\",\n#    \"\\n\\n1. Add the two numbers together: 10 + 12 = 22 \\n2. Subtract one of the numbers: 22 - 10 = 12 \\n3. Multiply the two numbers together: 12 x 12 = 144 \\n4. Divide the first number by the result: 144 / 10 = 14.4 \\n5. Add the initial two numbers together again: 14.4 + 12 = 26.4 \\n6. Subtract 2: 26.4 - 2 = 24\",\n# ]\n</code></pre> <p>Beware that in this case the output of the model is a NumPy array. So if you want to concatenate the prompt to the result you have to use <code>numpy.char.add</code>:</p> <pre><code>import numpy as np\n\nnew_prompts = np.char.add(prompts, results)\nprint(new_prompts)\n\n# [\n#     \"Use these numbers and basic arithmetic to get 24 as a result:\\n\\nInput: [1, 2, 3]\\nSteps:\\n1. 1 + 2 x 3 = 7\\n2. 7 + 3 x 4 = 19\\n3. 19 + 5 = 24\",\n#     \"Use these numbers and basic arithmetic to get 24 as a result:\\n\\nInput: [5, 9, 7]\\nSteps:\\n1. Add the three numbers together: 5 + 9 + 7 = 21\\n2. Subtract 21 from 24: 24 - 21 = 3\\n3. Multiply the remaining number by itself: 3 x 3 = 9\\n4. Add the number with the multiplication result: 21 + 9 = 24\",\n#    \"'Use these numbers and basic arithmetic to get 24 as a result:\\n\\nInput: [10, 12]\\nSteps:\\n\\n1. Add the two numbers together: 10 + 12 = 22 \\n2. Subtract one of the numbers: 22 - 10 = 12 \\n3. Multiply the two numbers together: 12 x 12 = 144 \\n4. Divide the first number by the result: 144 / 10 = 14.4 \\n5. Add the initial two numbers together again: 14.4 + 12 = 26.4 \\n6. Subtract 2: 26.4 - 2 = 24\",\n# ]\n</code></pre> <p>You can also ask for several samples for a single prompt:</p> <pre><code>from outlines import models\nfrom outlines import text\n\n\n@text.prompt\ndef template(input_numbers):\n    \"\"\"Use these numbers and basic arithmetic to get 24 as a result:\n\n    Input: {{ input_numbers }}\n    Steps:\"\"\"\n\n\nmodel = models.openai(\"text-davinci-003\")\nresults = model(template([1, 2, 3]), samples=3, stop_at=[\"\\n2\"])\nprint(results.shape)\n# (3,)\n\nprint(results)\n# [\n#     ' \\n1. Subtract 1 from 3',\n#     '\\n1. Add the three numbers: 1 + 2 + 3 = 6',\n#     ' (1 + 3) x (2 + 2) = 24'\n# ]\n</code></pre> <p>Or ask for several samples for an array of prompts. In this case the last dimension is the sample dimension:</p> <pre><code>from outlines import models\nfrom outlines import text\n\n\n@text.prompt\ndef template(input_numbers):\n    \"\"\"Use these numbers and basic arithmetic to get 24 as a result:\n\n    Input: {{ input_numbers }}\n    Steps:\"\"\"\n\n\nprompts = [template([1, 2, 3]), template([5, 9, 7]), template([10, 12])]\n\nmodel = models.openai(\"text-davinci-003\")\nresults = model(prompts, samples=2, stop_at=[\"\\n2\"])\nprint(results.shape)\n# (3, 2)\n\nprint(results)\n# [\n#     ['\\n1. Add the numbers: 1 + 2 + 3 = 6', ' (3 * 2) - 1 = 5\\n        5 * 4 = 20\\n        20 + 4 = 24'],\n#     ['\\n\\n1. (5 + 9) x 7 =  56', '\\n1. 5 x 9 = 45'],\n#     [' \\n1. Add the two numbers together: 10 + 12 = 22', '\\n1. Add 10 + 12']\n# ]\n</code></pre> <p>You may find this useful, e.g., to implement Tree of Thoughts.</p> <p>Note</p> <p>Outlines provides an <code>@outlines.vectorize</code> decorator that you can use on any <code>async</code> python function. This can be useful for instance when you call a remote API within your workflow.</p>"},{"location":"reference/prompting/","title":"Prompting techniques","text":""},{"location":"reference/prompting/#prompt-templating","title":"Prompt templating","text":"<p>Outlines provides a powerful domain-specific language to write and manage prompts, via what we call prompt functions. Prompt functions are Python functions that contain a template for the prompt in their docstring, and their arguments correspond to the variables used in the prompt. When called, a prompt function returns the template rendered with the values of the arguments:</p> <pre><code>import outlines.text as text\n\n@text.prompt\ndef greetings(name, question):\n    \"\"\"Hello, {{ name }}!\n    {{ question }}\n    \"\"\"\n\nprompt = greetings(\"user\", \"How are you?\")\n# Hello, user!\n# How are you?\n</code></pre> <p>Outlines uses the Jinja templating engine to render prompts, which allows to easily compose complex prompts. No need for extra abstractions to write a prompt with few-shot examples, Jinja can handle that:</p> CodeOutput <pre><code>import outlines.text as text\n\n@text.prompt\ndef few_shots(instructions, examples, question):\n    \"\"\"\"{{ instructions }}\n\n    {% for examples in examples %}\n    Q: {{ example.question }}\n    A: {{ example.answer }}\n    {% endfor %}\n    Q: {{ question }}\n    \"\"\"\n\nprompt = few_shots(question, examples, question)\n</code></pre> <p>Something</p> <p>Please refer to the <code>Jinja documentation &lt;https://jinja.palletsprojects.com/en/3.1.x/&gt;</code>_ for more information about the syntax of the templating language. The Jinja syntax is powerful, and we recommend you take some time to read their documentation if building your prompts requires complex logic involving for instance loops and conditionals.</p>"},{"location":"reference/prompting/#tools","title":"Tools","text":"<p>Several projects (e.g.<code>Toolformer &lt;https://arxiv.org/abs/2302.04761&gt;</code>, <code>ViperGPT &lt;https://viper.cs.columbia.edu/&gt;</code>, <code>AutoGPT &lt;https://github.com/Significant-Gravitas/Auto-GPT&gt;</code>_, etc.) have shown that we can \"teach\" language models to use external functions by describing what these functions do in the prompt. In these projects the same information is often repeated twice: the function implementation, name, docstring, or arguments are copy-pasted in the prompt. This is cumbersome and error prone; you can directly pull this information from within an Outlines prompt function:</p> CodeOutput <pre><code>import outlines.text as text\n\ndef my_tool(arg1: str, arg2: int):\n    \"\"\"Tool description.\n\n    The rest of the docstring\n    \"\"\"\n    pass\n\n@text.prompt\ndef tool_prompt(question, tool):\n    \"\"\"{{ question }}\n\n    COMMANDS\n    1. {{ tool | name }}: {{ tool | description }}, args: {{ tool | args }}\n\n    {{ tool | source }}\n    \"\"\"\n\ntool_prompt(\"Can you do something?\", my_tool)\n# Can you do something?\n#\n# COMMANDS\n# 1. my_tool: Tool description, args: arg1:str, arg2:int\n#\n# def my_tool(arg1: str, arg2: int):\n#     \"\"\"Tool description.\n#\n#     The rest of the docstring\n#     \"\"\"\n#     pass\n</code></pre> <p>Something</p>"},{"location":"reference/prompting/#json-response-format","title":"JSON response format","text":"<p>To build reliable chains with language models we often need to instruct them the format in which we would like them to return their response. Again the information is often repeated twice between creating the parsing function, and writing the desired schema in the prompt. You can directly pull the JSON schema of a pydantic model, or pretty print a dictionary from within an Outlines prompt function</p> Code <pre><code>from pydantic import BaseModel, Field\n\nimport outlines.text as text\n\nclass MyResponse(BaseModel):\n    field1: int = Field(description=\"an int\")\n    field2: str\n\n@text.prompt\ndef my_prompt(response_model):\n    \"\"\"{{ response_model | schema }}\"\"\"\n\nmy_prompt(MyResponse)\n# {\n#   \"field1\": \"an int\",\n#   \"field2\": \"&lt;field2&gt;\"\n# }\n</code></pre> Output <pre><code>response = {\n    \"field1\": \"&lt;field1&gt;\",\n    \"field2\": \"a string\"\n}\n\nmy_prompt(MyResponse)\n# {\n#   \"field1\": \"&lt;field1&gt;\",\n#   \"field2\": \"a string\"\n# }\n</code></pre>"},{"location":"reference/regex/","title":"Regular expressions","text":""},{"location":"reference/types/","title":"Type constraints","text":"<p>We can ask completions to be restricted to valid integers or floating-point numbers using the <code>type</code> keyword argument, respectively with the \u201cint\u201d or \u201cfloat\u201d value:</p> <pre><code>import outlines.models as models\n\ncomplete = models.text_completion.openai(\"text-davinci-002\")\nanswer = complete(\n    \"When I was 6 my sister was half my age. Now I\u2019m 70 how old is my sister?\",\n    type=\"int\"\n)\n</code></pre>"}]}